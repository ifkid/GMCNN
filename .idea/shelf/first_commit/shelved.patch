Index: emb/karate.emb
===================================================================
--- emb/karate.emb	(date 1558009865000)
+++ emb/karate.emb	(date 1558009865000)
@@ -1,35 +0,0 @@
-34 128
-1 -0.014876 -0.229356 0.023806 0.067841 0.090910 -0.144880 0.150130 0.075787 0.046873 -0.025290 -0.095061 0.084067 0.022556 -0.197516 0.051331 -0.241755 0.004764 -0.232580 -0.031161 -0.013199 0.233664 -0.011230 0.140365 0.096059 0.068695 -0.165448 -0.100326 -0.012106 0.011358 0.170880 -0.012065 0.044922 -0.139547 -0.037438 -0.095815 -0.138876 0.183298 0.116599 -0.055275 -0.080746 -0.100899 0.053135 0.141539 0.179031 -0.131575 0.127407 0.099880 -0.064466 -0.029267 0.157667 -0.028006 0.174557 -0.022628 0.148424 -0.174256 -0.179070 -0.059111 0.088243 0.086486 -0.033525 0.117324 -0.099924 -0.026303 0.151631 -0.063171 -0.079035 -0.073884 0.059158 0.136396 -0.156904 -0.089332 0.054849 -0.108355 -0.044886 0.131026 -0.063695 0.086190 0.014780 -0.002258 -0.120929 -0.115947 -0.029460 0.023848 -0.102442 -0.111003 0.058831 0.121057 0.008298 -0.056379 0.072060 0.048521 -0.006848 0.087435 -0.054377 -0.153402 0.207793 -0.070880 -0.107374 0.120661 -0.008031 -0.145239 0.157040 -0.122576 -0.109220 -0.100591 0.017339 0.173152 0.001466 0.015540 -0.049476 0.057062 0.019142 -0.027390 0.016272 -0.091249 -0.008336 -0.042350 0.178391 -0.066354 -0.029698 -0.038339 -0.048144 0.134590 -0.234350 -0.217189 0.074112 0.057659 0.201265
-34 -0.043236 -0.184561 0.086898 0.155603 0.126673 -0.103197 0.194487 -0.020264 -0.096339 0.018373 -0.113667 0.012346 0.015797 -0.198612 -0.094771 -0.205460 0.049062 -0.081919 0.027739 0.088356 0.151644 -0.009231 0.201849 0.050836 -0.023565 -0.149574 -0.049034 -0.044401 -0.037658 0.224190 -0.010381 0.071883 -0.170195 0.034827 -0.167335 -0.075119 0.218483 0.090103 -0.035649 -0.134331 -0.168675 0.210952 0.128298 0.087990 -0.063460 0.092851 0.023417 -0.128993 -0.074394 0.129745 0.093197 0.249170 -0.001558 0.100948 -0.161581 -0.110734 -0.069279 0.041938 0.008046 0.054898 0.023925 -0.086794 -0.147930 0.155933 0.051813 -0.310828 -0.066930 -0.007734 0.062363 -0.038964 0.059932 0.004334 -0.072056 0.082316 -0.050440 -0.114736 0.039152 0.079540 0.074609 0.144051 -0.082203 -0.145513 0.004876 -0.252469 0.005756 0.245110 -0.101092 0.065562 -0.171087 0.005519 -0.005527 0.094499 0.054681 -0.044853 -0.037731 0.285537 -0.169364 0.082962 0.097452 -0.184904 -0.246170 0.201754 -0.079283 -0.180332 -0.009282 0.074115 0.221057 0.033268 0.021976 0.014238 0.086000 -0.090141 -0.070816 -0.046511 -0.025651 -0.059833 -0.086892 0.139139 -0.074664 0.109995 -0.096493 -0.132756 0.014005 -0.184265 0.008678 0.209333 0.024216 0.046068
-33 -0.037151 -0.193182 0.077220 0.145653 0.123061 -0.110840 0.185281 -0.003266 -0.074293 0.013228 -0.108248 0.023448 0.017491 -0.203065 -0.068931 -0.211348 0.036744 -0.099373 0.013532 0.072291 0.169995 -0.009909 0.193386 0.056882 -0.015445 -0.150086 -0.060747 -0.037020 -0.029384 0.216726 -0.015124 0.072654 -0.162907 0.029320 -0.163866 -0.086833 0.208782 0.090374 -0.040612 -0.127639 -0.159738 0.193453 0.128516 0.092407 -0.070034 0.102198 0.026696 -0.120459 -0.073511 0.130051 0.074555 0.242593 0.000266 0.103154 -0.166959 -0.113425 -0.069956 0.046228 0.019883 0.047479 0.043045 -0.096340 -0.130470 0.151841 0.041735 -0.279075 -0.063606 0.000315 0.068115 -0.050509 0.045616 0.007611 -0.069524 0.064300 -0.031539 -0.102848 0.052158 0.066086 0.064956 0.109075 -0.078587 -0.132586 0.004139 -0.232968 -0.010310 0.221729 -0.075292 0.054786 -0.151444 0.012827 0.007983 0.083758 0.057488 -0.049456 -0.048113 0.276285 -0.158635 0.053341 0.102118 -0.161438 -0.231241 0.187751 -0.082484 -0.163907 -0.014804 0.060307 0.209479 0.030831 0.023088 0.002885 0.075017 -0.076178 -0.068085 -0.047820 -0.030464 -0.050482 -0.082474 0.133255 -0.076936 0.086339 -0.085110 -0.118404 0.027944 -0.186448 -0.009368 0.183909 0.023062 0.057413
-3 -0.028561 -0.205411 0.056815 0.105204 0.107464 -0.127154 0.173082 0.029493 -0.026830 -0.005606 -0.101965 0.047164 0.018824 -0.210376 -0.017246 -0.231535 0.027110 -0.170617 -0.010041 0.031033 0.197806 -0.006828 0.168036 0.082497 0.027082 -0.157874 -0.080517 -0.019565 -0.000477 0.203291 -0.015622 0.058068 -0.156901 0.000501 -0.131919 -0.116712 0.199355 0.106443 -0.044448 -0.103124 -0.131104 0.115538 0.131251 0.140924 -0.098906 0.113108 0.063865 -0.097658 -0.058951 0.142671 0.027667 0.202568 -0.018042 0.131458 -0.174052 -0.145032 -0.066079 0.062356 0.048391 0.014275 0.072580 -0.089248 -0.084891 0.152485 -0.006358 -0.189735 -0.068464 0.033716 0.102633 -0.109285 -0.020809 0.035645 -0.092487 0.013135 0.052743 -0.089519 0.070987 0.045890 0.035846 -0.001593 -0.098935 -0.083680 0.012137 -0.175651 -0.054943 0.149670 0.019547 0.031285 -0.114596 0.042073 0.024935 0.042274 0.071669 -0.057264 -0.105576 0.250713 -0.121465 -0.015699 0.109578 -0.090905 -0.194813 0.173052 -0.108581 -0.135182 -0.051883 0.047640 0.190100 0.018619 0.022392 -0.026230 0.077392 -0.031160 -0.044318 -0.011937 -0.059609 -0.030396 -0.066347 0.157975 -0.064017 0.032325 -0.066449 -0.089884 0.073737 -0.207898 -0.113919 0.137631 0.044525 0.131904
-2 -0.014195 -0.206891 0.039293 0.085593 0.095233 -0.124536 0.156750 0.051983 0.001926 -0.015428 -0.094487 0.068283 0.022594 -0.197800 0.013198 -0.227624 0.008343 -0.180030 -0.019804 0.011482 0.205034 -0.006173 0.155285 0.082322 0.037030 -0.151400 -0.087726 -0.019770 0.005120 0.180771 -0.010223 0.048888 -0.152389 -0.009914 -0.112808 -0.117258 0.191241 0.099122 -0.047605 -0.088254 -0.111873 0.091852 0.139276 0.156382 -0.111017 0.116605 0.073830 -0.084164 -0.043307 0.137957 0.005370 0.179475 -0.012714 0.137657 -0.170627 -0.155057 -0.059229 0.074431 0.059581 -0.005058 0.088855 -0.090385 -0.056987 0.144462 -0.028853 -0.128125 -0.068681 0.038364 0.110911 -0.127508 -0.048368 0.043526 -0.091543 -0.015820 0.086858 -0.069705 0.077332 0.030236 0.014120 -0.050888 -0.104370 -0.061443 0.009650 -0.136330 -0.077768 0.111878 0.061145 0.022060 -0.081506 0.053774 0.040389 0.023848 0.076200 -0.059771 -0.129641 0.226712 -0.095737 -0.059849 0.105248 -0.044966 -0.162578 0.155537 -0.106955 -0.124103 -0.077882 0.022313 0.180785 0.018175 0.012676 -0.037498 0.061517 -0.005846 -0.034860 -0.004758 -0.074445 -0.020110 -0.054517 0.165222 -0.071541 0.005364 -0.052736 -0.073605 0.103373 -0.216421 -0.150806 0.105295 0.055725 0.159106
-4 -0.014641 -0.203711 0.034961 0.085020 0.095704 -0.131868 0.150660 0.056755 0.011886 -0.016651 -0.089766 0.067894 0.024813 -0.194787 0.020632 -0.224535 0.010860 -0.193962 -0.023700 0.008739 0.211455 -0.009271 0.145376 0.082361 0.047274 -0.151999 -0.089413 -0.014303 0.007932 0.174257 -0.014606 0.045983 -0.143194 -0.019511 -0.106699 -0.121798 0.188384 0.107257 -0.049663 -0.084457 -0.107101 0.077382 0.134446 0.157648 -0.108230 0.113253 0.079461 -0.079183 -0.043650 0.138382 0.001375 0.174463 -0.015088 0.134528 -0.166295 -0.153995 -0.057372 0.071027 0.069189 -0.016081 0.099031 -0.091601 -0.048610 0.142226 -0.029463 -0.114172 -0.072477 0.043485 0.114788 -0.127927 -0.054399 0.046114 -0.091841 -0.023762 0.094924 -0.068927 0.077757 0.029653 0.009168 -0.056918 -0.098812 -0.055162 0.015343 -0.122034 -0.078487 0.094937 0.074184 0.015670 -0.075195 0.058859 0.034632 0.014311 0.071581 -0.055793 -0.125368 0.216919 -0.088715 -0.066234 0.104393 -0.041869 -0.155419 0.150385 -0.102527 -0.107840 -0.081059 0.026027 0.165621 0.010435 0.014196 -0.037497 0.064977 -0.004085 -0.032967 0.003786 -0.069743 -0.013293 -0.051573 0.158915 -0.062837 -0.001615 -0.044520 -0.061123 0.099564 -0.209667 -0.159169 0.095354 0.052871 0.161271
-32 -0.028685 -0.213070 0.047301 0.108818 0.109415 -0.128147 0.176653 0.027303 -0.025159 -0.004165 -0.105345 0.053321 0.018802 -0.204727 -0.014004 -0.228356 0.026948 -0.172561 -0.012788 0.035630 0.203829 -0.004683 0.168709 0.078730 0.022190 -0.164596 -0.086578 -0.023510 -0.008559 0.203468 -0.010573 0.059740 -0.148701 -0.003012 -0.134304 -0.112708 0.203686 0.109460 -0.050573 -0.109072 -0.136512 0.126445 0.128359 0.133714 -0.103106 0.115873 0.066037 -0.090537 -0.055603 0.139785 0.027898 0.210594 -0.017086 0.130375 -0.174136 -0.149086 -0.063542 0.062951 0.050607 0.008530 0.075894 -0.092578 -0.084852 0.157976 -0.014030 -0.184613 -0.072039 0.032966 0.101139 -0.104970 -0.026077 0.038771 -0.092542 0.011978 0.047402 -0.082716 0.062619 0.046412 0.032246 0.000488 -0.098291 -0.080592 0.019979 -0.166354 -0.054311 0.148484 0.019857 0.032152 -0.108305 0.035798 0.024141 0.041786 0.075691 -0.055285 -0.095965 0.244279 -0.114245 -0.016270 0.102579 -0.095522 -0.187384 0.167964 -0.106744 -0.134279 -0.050422 0.048754 0.190782 0.013687 0.018321 -0.018169 0.071399 -0.032380 -0.043966 -0.010604 -0.056195 -0.028461 -0.064954 0.154093 -0.059146 0.030605 -0.064815 -0.091060 0.073829 -0.205478 -0.104267 0.131276 0.034881 0.128345
-24 -0.029315 -0.189772 0.068621 0.134244 0.113263 -0.111526 0.179366 0.006049 -0.055758 0.008379 -0.101671 0.030253 0.020045 -0.200031 -0.051633 -0.212913 0.031469 -0.122085 0.007588 0.056852 0.173856 -0.008787 0.179263 0.067520 0.002459 -0.148246 -0.065914 -0.037638 -0.025133 0.208168 -0.016622 0.063915 -0.162850 0.012209 -0.148385 -0.094801 0.212578 0.094363 -0.044566 -0.125276 -0.148900 0.172375 0.128087 0.110670 -0.080469 0.106983 0.040901 -0.105512 -0.063393 0.132020 0.064159 0.230891 -0.009803 0.113915 -0.160158 -0.123251 -0.063602 0.050361 0.029889 0.029696 0.051962 -0.091280 -0.110055 0.153433 0.019818 -0.245125 -0.068114 0.014500 0.076046 -0.066689 0.022533 0.020194 -0.083352 0.046498 -0.001833 -0.098371 0.049496 0.056079 0.051472 0.077022 -0.081356 -0.108193 0.010358 -0.201614 -0.025917 0.187518 -0.045225 0.052643 -0.135496 0.023657 0.010510 0.068199 0.063236 -0.049041 -0.063124 0.265099 -0.134215 0.031199 0.096965 -0.135953 -0.212055 0.182376 -0.084096 -0.150302 -0.031544 0.058467 0.196833 0.024433 0.022537 0.000038 0.078107 -0.060734 -0.059215 -0.031952 -0.043136 -0.043312 -0.075321 0.136955 -0.064975 0.072500 -0.075805 -0.107042 0.043740 -0.190984 -0.039007 0.166575 0.030490 0.083785
-14 -0.027129 -0.198493 0.061423 0.105227 0.107909 -0.116048 0.165620 0.021164 -0.039371 0.000798 -0.105297 0.046454 0.020127 -0.201215 -0.025640 -0.218301 0.026254 -0.145145 -0.003993 0.036285 0.189837 -0.001548 0.169266 0.074046 0.012566 -0.145471 -0.070417 -0.019415 -0.011722 0.199796 -0.013031 0.061321 -0.155937 0.010501 -0.129864 -0.105259 0.194787 0.093700 -0.043040 -0.106005 -0.136448 0.129663 0.130622 0.132160 -0.088718 0.109683 0.051199 -0.096384 -0.058666 0.135832 0.033188 0.202058 -0.010150 0.124697 -0.166948 -0.134049 -0.066910 0.061093 0.035829 0.016422 0.067523 -0.093323 -0.088033 0.141622 0.001714 -0.188714 -0.062100 0.020227 0.088135 -0.091097 -0.010059 0.031509 -0.082984 0.023107 0.036634 -0.082443 0.064717 0.047751 0.036376 0.020414 -0.091274 -0.092302 0.010257 -0.170598 -0.046212 0.162372 -0.004781 0.033150 -0.109946 0.033680 0.021587 0.046615 0.063440 -0.057116 -0.089360 0.243070 -0.123019 0.000268 0.100286 -0.097457 -0.190594 0.162703 -0.092669 -0.130177 -0.047080 0.045170 0.186444 0.027099 0.014230 -0.019465 0.063650 -0.042349 -0.049969 -0.016807 -0.047529 -0.034889 -0.069347 0.142300 -0.068355 0.042539 -0.065908 -0.092345 0.059285 -0.191807 -0.077834 0.134570 0.036721 0.108272
-30 -0.029805 -0.195961 0.067188 0.138667 0.112269 -0.110647 0.186154 -0.000130 -0.064409 0.011206 -0.110666 0.029180 0.017283 -0.202088 -0.059766 -0.216968 0.035102 -0.117452 0.007007 0.065951 0.173340 -0.011271 0.187594 0.059655 -0.000878 -0.153483 -0.060203 -0.036643 -0.028904 0.212611 -0.014515 0.062677 -0.160968 0.015566 -0.153099 -0.088347 0.209994 0.097673 -0.040055 -0.127585 -0.150775 0.176175 0.128623 0.109289 -0.077416 0.104198 0.038445 -0.114737 -0.066683 0.134714 0.065543 0.233909 -0.003867 0.105055 -0.159656 -0.122573 -0.070121 0.050437 0.028871 0.034093 0.049062 -0.093662 -0.113433 0.153901 0.031128 -0.253415 -0.068951 0.007318 0.077145 -0.065763 0.028671 0.015899 -0.078109 0.049580 -0.008314 -0.102949 0.051671 0.064192 0.055459 0.087741 -0.088441 -0.117364 0.004683 -0.212010 -0.019745 0.199981 -0.053953 0.052947 -0.143431 0.021250 0.012143 0.067632 0.060727 -0.045437 -0.058646 0.265112 -0.144213 0.040904 0.099171 -0.143674 -0.210708 0.183055 -0.085558 -0.160468 -0.027837 0.057454 0.201077 0.025756 0.017544 -0.000789 0.073068 -0.067587 -0.063775 -0.036334 -0.034737 -0.045930 -0.079362 0.137832 -0.069735 0.079231 -0.082655 -0.107603 0.038808 -0.186957 -0.032019 0.173386 0.033657 0.076414
-6 0.002078 -0.268463 -0.003595 0.061690 0.088494 -0.169720 0.158606 0.118915 0.104116 -0.043795 -0.095630 0.123569 0.022248 -0.218749 0.115644 -0.277518 -0.014723 -0.313166 -0.060689 -0.047875 0.288100 -0.020190 0.132303 0.118331 0.117887 -0.200003 -0.132013 -0.017603 0.019659 0.177378 -0.006264 0.034392 -0.144590 -0.073253 -0.077074 -0.172127 0.207134 0.149657 -0.083728 -0.071973 -0.090280 0.020017 0.167903 0.226158 -0.175951 0.159062 0.152982 -0.053364 -0.014949 0.189750 -0.065455 0.174992 -0.030634 0.172406 -0.186966 -0.233308 -0.063218 0.106419 0.135712 -0.083162 0.167874 -0.111255 0.019922 0.172214 -0.106326 -0.006885 -0.089321 0.084059 0.181457 -0.217192 -0.148822 0.081893 -0.144617 -0.089499 0.217508 -0.047376 0.106679 -0.000777 -0.035308 -0.219362 -0.139127 0.023513 0.039169 -0.045416 -0.159940 -0.008134 0.218900 -0.008446 -0.026698 0.104784 0.072880 -0.043530 0.111360 -0.052411 -0.210585 0.205173 -0.034588 -0.189699 0.137564 0.055408 -0.113309 0.151570 -0.151678 -0.094919 -0.158202 -0.006576 0.163549 -0.015275 0.019636 -0.075174 0.055843 0.066792 -0.019707 0.035878 -0.116778 0.019480 -0.031952 0.205222 -0.063552 -0.076776 -0.017083 -0.022952 0.203400 -0.281455 -0.323782 0.030069 0.078347 0.272427
-28 -0.031933 -0.197667 0.064274 0.115887 0.109673 -0.112831 0.172487 0.015226 -0.045440 0.000922 -0.100285 0.041999 0.019360 -0.195612 -0.036887 -0.218494 0.025783 -0.140635 0.001926 0.047809 0.179597 -0.007532 0.172192 0.069171 0.007049 -0.146625 -0.074777 -0.025957 -0.010022 0.198732 -0.015366 0.065234 -0.151395 0.012260 -0.137601 -0.103174 0.199766 0.096171 -0.039490 -0.110485 -0.138451 0.140136 0.128419 0.120470 -0.087535 0.103411 0.041446 -0.101929 -0.063897 0.129588 0.039896 0.211609 -0.006230 0.115810 -0.161193 -0.129435 -0.065928 0.050153 0.031238 0.023657 0.060655 -0.092088 -0.094960 0.147079 0.012121 -0.212376 -0.067530 0.022987 0.080594 -0.086674 0.001573 0.025931 -0.081351 0.024362 0.018821 -0.089987 0.060000 0.054546 0.038437 0.043187 -0.088256 -0.103463 0.010522 -0.187656 -0.037527 0.173049 -0.014719 0.041709 -0.121271 0.031382 0.019447 0.054744 0.061988 -0.056360 -0.078127 0.246409 -0.128369 0.010664 0.099073 -0.115142 -0.199420 0.175094 -0.094355 -0.143670 -0.035543 0.051635 0.192875 0.027025 0.020782 -0.017472 0.072300 -0.045097 -0.055823 -0.026098 -0.047309 -0.039429 -0.071946 0.141525 -0.068355 0.048830 -0.069811 -0.097455 0.053574 -0.189748 -0.071610 0.145047 0.036588 0.100697
-9 -0.028741 -0.211076 0.054543 0.113454 0.112192 -0.125957 0.179462 0.027106 -0.028922 -0.004419 -0.107562 0.053037 0.021575 -0.217922 -0.024299 -0.237413 0.024300 -0.160490 -0.008231 0.041769 0.199263 -0.003726 0.183270 0.076422 0.018500 -0.159869 -0.083470 -0.031711 -0.011421 0.213216 -0.012059 0.066545 -0.164235 0.000059 -0.135374 -0.114367 0.211365 0.103426 -0.050803 -0.107760 -0.144874 0.131460 0.138487 0.143084 -0.099800 0.116327 0.059543 -0.104551 -0.061709 0.139687 0.031561 0.216445 -0.014551 0.132551 -0.174128 -0.147784 -0.069049 0.061083 0.044929 0.013560 0.073751 -0.095200 -0.094416 0.159048 0.001244 -0.203643 -0.068519 0.027594 0.098585 -0.101190 -0.011652 0.036458 -0.086973 0.017561 0.045833 -0.092934 0.063652 0.055411 0.034048 0.017822 -0.095928 -0.093888 0.008178 -0.187700 -0.052890 0.162931 0.005551 0.035677 -0.122494 0.039770 0.026802 0.046470 0.076745 -0.054114 -0.100411 0.253836 -0.121971 -0.008027 0.105489 -0.097454 -0.201514 0.171386 -0.106460 -0.139697 -0.050451 0.044910 0.193437 0.025752 0.016820 -0.024537 0.076425 -0.037152 -0.045500 -0.020131 -0.057735 -0.035467 -0.071962 0.158357 -0.070841 0.044458 -0.070259 -0.096717 0.068233 -0.206222 -0.099504 0.146867 0.042443 0.123567
-8 -0.017865 -0.211108 0.047087 0.089959 0.095930 -0.129200 0.162559 0.046570 -0.004375 -0.008637 -0.098684 0.061683 0.021941 -0.204175 0.012324 -0.233112 0.015407 -0.184187 -0.015316 0.016029 0.207185 -0.006227 0.154479 0.078815 0.037447 -0.147990 -0.084808 -0.012550 0.005331 0.188156 -0.016680 0.052601 -0.150952 -0.007804 -0.117620 -0.122416 0.198871 0.103815 -0.043373 -0.089026 -0.118659 0.092678 0.133247 0.160776 -0.106863 0.122023 0.070368 -0.087064 -0.050659 0.143238 0.005149 0.183299 -0.011017 0.141231 -0.177581 -0.152867 -0.065982 0.068455 0.058193 -0.006387 0.090205 -0.095814 -0.063365 0.149157 -0.020920 -0.143041 -0.067499 0.037596 0.111694 -0.126333 -0.047938 0.041035 -0.095825 -0.012171 0.082612 -0.074008 0.073319 0.033771 0.017426 -0.041337 -0.098758 -0.070711 0.009757 -0.141484 -0.076250 0.121256 0.057527 0.022052 -0.087179 0.051636 0.038312 0.022068 0.071565 -0.062129 -0.121745 0.227285 -0.099703 -0.054338 0.100271 -0.059531 -0.161960 0.151497 -0.109298 -0.116800 -0.066790 0.026844 0.177323 0.011312 0.014639 -0.038243 0.063205 -0.009735 -0.033455 0.000628 -0.073667 -0.017436 -0.058714 0.157400 -0.066595 0.007332 -0.054345 -0.075661 0.096529 -0.204896 -0.138533 0.104377 0.054124 0.149600
-7 0.004536 -0.254080 -0.005178 0.057268 0.083031 -0.160793 0.152251 0.109620 0.097156 -0.041831 -0.095120 0.116381 0.023259 -0.211618 0.107966 -0.263637 -0.010155 -0.292228 -0.062469 -0.041445 0.275748 -0.017947 0.131967 0.117549 0.107937 -0.190531 -0.131160 -0.015198 0.016388 0.167363 -0.002318 0.040055 -0.140953 -0.069910 -0.079856 -0.160896 0.193364 0.132179 -0.076877 -0.067679 -0.087655 0.021391 0.153731 0.214042 -0.160299 0.151596 0.140628 -0.047208 -0.016141 0.176074 -0.061066 0.166253 -0.026844 0.168321 -0.184576 -0.212191 -0.057258 0.100167 0.125962 -0.066334 0.153393 -0.106310 0.015351 0.163925 -0.100897 -0.008578 -0.081105 0.081465 0.172725 -0.203431 -0.141983 0.080377 -0.138018 -0.082646 0.200963 -0.042303 0.096555 -0.001276 -0.029828 -0.206510 -0.126383 0.019984 0.033506 -0.044342 -0.150970 0.000560 0.194671 -0.005433 -0.028069 0.098008 0.067910 -0.039512 0.106683 -0.054327 -0.194675 0.199312 -0.033676 -0.172281 0.121926 0.047836 -0.112250 0.145785 -0.146147 -0.089646 -0.142902 -0.010297 0.153126 -0.016220 0.020992 -0.066057 0.052915 0.063302 -0.019462 0.027044 -0.111650 0.011834 -0.033692 0.189723 -0.062758 -0.066532 -0.020178 -0.020366 0.188008 -0.256947 -0.294357 0.027996 0.071474 0.257231
-11 -0.011104 -0.238662 0.017651 0.075369 0.092326 -0.150237 0.153379 0.086072 0.059588 -0.030736 -0.095822 0.094182 0.019264 -0.201241 0.068715 -0.252125 -0.001463 -0.247265 -0.041062 -0.019052 0.248472 -0.015812 0.142417 0.101158 0.078408 -0.176218 -0.110534 -0.015693 0.006325 0.172081 -0.007481 0.042705 -0.139252 -0.048455 -0.091122 -0.140135 0.198064 0.126991 -0.064129 -0.073303 -0.096557 0.056212 0.147887 0.185500 -0.139213 0.137412 0.115886 -0.059514 -0.031032 0.163974 -0.028957 0.175716 -0.019992 0.148652 -0.174109 -0.192077 -0.060479 0.089457 0.094409 -0.044620 0.125362 -0.103718 -0.012229 0.158348 -0.069101 -0.065849 -0.080717 0.060038 0.149384 -0.170787 -0.098626 0.059951 -0.119967 -0.054091 0.151870 -0.056308 0.087860 0.008745 -0.011980 -0.135536 -0.113264 -0.013711 0.025772 -0.088830 -0.123062 0.048005 0.141393 0.006782 -0.051220 0.078137 0.050331 -0.016018 0.092582 -0.048781 -0.161419 0.206455 -0.058673 -0.120110 0.115591 0.007645 -0.128734 0.152522 -0.128197 -0.101811 -0.113094 0.007255 0.165345 -0.009922 0.018239 -0.054996 0.055853 0.032588 -0.030402 0.013102 -0.091771 -0.001437 -0.045370 0.179492 -0.062742 -0.036980 -0.031203 -0.044651 0.151537 -0.240748 -0.230398 0.058123 0.057474 0.209633
-26 -0.023020 -0.198035 0.052784 0.110189 0.107711 -0.121333 0.171473 0.021895 -0.032644 -0.004038 -0.101837 0.043534 0.021820 -0.197700 -0.022046 -0.215946 0.026357 -0.152653 -0.007251 0.039833 0.193019 -0.007593 0.170483 0.074832 0.018060 -0.156527 -0.080434 -0.030780 -0.016583 0.199059 -0.008473 0.058927 -0.147682 -0.004470 -0.132989 -0.106190 0.197362 0.098274 -0.048066 -0.107485 -0.132600 0.128865 0.123421 0.123282 -0.093707 0.113453 0.053405 -0.093328 -0.056185 0.139182 0.038118 0.209039 -0.012684 0.115021 -0.161031 -0.138554 -0.060245 0.059080 0.041824 0.012730 0.069979 -0.091850 -0.083236 0.151016 -0.000600 -0.187181 -0.064777 0.020779 0.092040 -0.088948 -0.010133 0.029883 -0.087822 0.018203 0.037744 -0.086909 0.054650 0.046275 0.033388 0.017380 -0.092644 -0.080701 0.014382 -0.165403 -0.043736 0.150511 -0.000230 0.035232 -0.110608 0.033784 0.020769 0.043880 0.070284 -0.044217 -0.083884 0.235181 -0.114249 -0.003872 0.101984 -0.097426 -0.181580 0.163574 -0.092290 -0.129696 -0.045518 0.044875 0.183125 0.017928 0.022999 -0.011724 0.063166 -0.037035 -0.048019 -0.016431 -0.049066 -0.030906 -0.062725 0.139018 -0.059069 0.036730 -0.063343 -0.088477 0.062320 -0.195873 -0.085636 0.136442 0.033857 0.105298
-5 -0.004594 -0.243196 0.005785 0.065577 0.090126 -0.153967 0.154449 0.088095 0.073435 -0.034911 -0.090112 0.105713 0.019047 -0.202371 0.080980 -0.256649 -0.007039 -0.264985 -0.047738 -0.023556 0.256849 -0.011095 0.131734 0.108428 0.090315 -0.180211 -0.114032 -0.017658 0.012227 0.170835 -0.010182 0.037963 -0.142614 -0.050532 -0.080946 -0.151261 0.194782 0.130602 -0.067567 -0.069304 -0.087982 0.037704 0.145634 0.197020 -0.148580 0.139400 0.123792 -0.058108 -0.025629 0.162819 -0.041917 0.169209 -0.018981 0.148528 -0.171418 -0.200696 -0.057636 0.091955 0.108952 -0.060386 0.133370 -0.104544 -0.001118 0.159968 -0.081236 -0.038192 -0.073945 0.064121 0.156134 -0.179419 -0.115688 0.067210 -0.123707 -0.069146 0.169208 -0.053796 0.087754 0.002529 -0.021773 -0.162555 -0.120582 -0.002197 0.026520 -0.068466 -0.132423 0.023636 0.163203 0.004269 -0.038202 0.083627 0.058034 -0.025884 0.094432 -0.049474 -0.176491 0.197759 -0.052375 -0.139631 0.121160 0.028213 -0.123731 0.142645 -0.131270 -0.097683 -0.127133 0.000257 0.157086 -0.010924 0.011315 -0.059939 0.049982 0.040890 -0.020221 0.020389 -0.098609 0.009737 -0.034123 0.182243 -0.056289 -0.046906 -0.025897 -0.031945 0.160402 -0.243433 -0.254963 0.048785 0.060837 0.227462
-20 -0.018263 -0.203119 0.044032 0.096708 0.093959 -0.128945 0.162148 0.043488 -0.007986 -0.009540 -0.096424 0.056310 0.026605 -0.195246 0.002651 -0.219970 0.010107 -0.169680 -0.017086 0.016119 0.202129 -0.011583 0.155043 0.076787 0.035868 -0.145195 -0.074411 -0.020829 0.000230 0.177906 -0.018463 0.050058 -0.145409 -0.009500 -0.112073 -0.111484 0.195414 0.098849 -0.049618 -0.097075 -0.119716 0.099080 0.128173 0.144992 -0.099394 0.113066 0.072445 -0.084550 -0.045729 0.134319 0.009104 0.186786 -0.007951 0.121093 -0.163520 -0.145787 -0.061469 0.061240 0.054717 -0.003879 0.090869 -0.089566 -0.062849 0.143736 -0.016331 -0.146340 -0.067350 0.033961 0.102259 -0.106254 -0.036726 0.037258 -0.089962 -0.005363 0.069195 -0.070990 0.071186 0.033198 0.022662 -0.022449 -0.093300 -0.071684 0.012518 -0.146311 -0.064636 0.120725 0.039939 0.025594 -0.091103 0.050819 0.034654 0.027948 0.068914 -0.054067 -0.108076 0.222217 -0.094144 -0.042908 0.096560 -0.056561 -0.162761 0.147145 -0.100650 -0.112914 -0.066399 0.030850 0.167822 0.012409 0.018386 -0.025553 0.059479 -0.014568 -0.037243 -0.002874 -0.064027 -0.021660 -0.052850 0.151709 -0.062739 0.015709 -0.051125 -0.065736 0.086524 -0.197463 -0.121859 0.108248 0.041986 0.136731
-29 -0.029217 -0.192326 0.054517 0.112169 0.109076 -0.116282 0.168307 0.012174 -0.039994 0.001809 -0.098568 0.043412 0.016036 -0.194299 -0.030963 -0.214521 0.024745 -0.142613 0.001282 0.044642 0.182777 -0.008179 0.165616 0.068198 0.007098 -0.151797 -0.073324 -0.030752 -0.013393 0.199615 -0.007267 0.059101 -0.145442 0.004420 -0.136301 -0.096027 0.197822 0.096257 -0.041719 -0.111380 -0.130995 0.141976 0.118482 0.111795 -0.082441 0.109905 0.046891 -0.095926 -0.059965 0.130532 0.041412 0.206435 -0.013023 0.116034 -0.163667 -0.124636 -0.056119 0.056049 0.037650 0.022077 0.060789 -0.090658 -0.095441 0.145759 0.002065 -0.204009 -0.069661 0.018610 0.086614 -0.079012 -0.004151 0.026184 -0.085070 0.029903 0.024578 -0.082741 0.056755 0.050838 0.039933 0.035330 -0.089804 -0.088362 0.014101 -0.179165 -0.036571 0.161763 -0.008797 0.033754 -0.115875 0.024550 0.017247 0.047270 0.064382 -0.047846 -0.073251 0.242764 -0.122172 0.010656 0.094378 -0.111195 -0.190514 0.167477 -0.090849 -0.132405 -0.037286 0.048096 0.180554 0.017383 0.021162 -0.010220 0.072079 -0.041447 -0.053255 -0.023962 -0.050160 -0.038334 -0.069232 0.134364 -0.059308 0.047420 -0.068231 -0.094575 0.052337 -0.190503 -0.066888 0.144212 0.031240 0.094808
-25 -0.035282 -0.206988 0.058312 0.120805 0.111692 -0.122058 0.177212 0.011913 -0.038791 0.000927 -0.108893 0.039183 0.017992 -0.200517 -0.036279 -0.230321 0.033777 -0.150954 0.003472 0.053558 0.195164 -0.010270 0.183469 0.076504 0.012123 -0.165208 -0.076565 -0.032481 -0.022911 0.213152 -0.007764 0.065099 -0.152898 0.001475 -0.147454 -0.099935 0.198991 0.104754 -0.047584 -0.118899 -0.147034 0.153900 0.125362 0.112688 -0.085567 0.114880 0.050960 -0.097374 -0.057590 0.140256 0.042312 0.225302 -0.010500 0.123003 -0.172205 -0.137021 -0.060565 0.052122 0.037197 0.026949 0.059353 -0.090034 -0.097365 0.150803 0.002949 -0.221907 -0.071696 0.022900 0.083551 -0.083490 0.000902 0.022435 -0.083410 0.035583 0.017835 -0.096991 0.054303 0.057029 0.046537 0.039250 -0.092136 -0.098973 0.014844 -0.194283 -0.038230 0.170998 -0.015587 0.034901 -0.129690 0.023305 0.011655 0.053895 0.073493 -0.053029 -0.081261 0.257914 -0.129617 0.012464 0.105111 -0.118070 -0.207143 0.181815 -0.098697 -0.145136 -0.036175 0.052328 0.198538 0.023094 0.025383 -0.012243 0.072062 -0.047573 -0.054932 -0.028930 -0.052588 -0.039808 -0.068874 0.144699 -0.067824 0.051959 -0.071644 -0.097496 0.054996 -0.205749 -0.075276 0.158864 0.029401 0.100663
-31 -0.024721 -0.196744 0.058380 0.113582 0.105830 -0.121297 0.167197 0.019532 -0.034359 0.000843 -0.097467 0.050559 0.023386 -0.199146 -0.025068 -0.221343 0.019521 -0.144300 -0.002430 0.040385 0.189624 -0.010693 0.165790 0.073749 0.016852 -0.147830 -0.069561 -0.021950 -0.011725 0.194129 -0.016407 0.062914 -0.153591 0.008004 -0.129214 -0.098710 0.194018 0.095890 -0.038745 -0.108933 -0.131078 0.135748 0.126969 0.121269 -0.088126 0.106741 0.049527 -0.099419 -0.056658 0.134461 0.031972 0.206586 -0.006793 0.116740 -0.162558 -0.134426 -0.061724 0.054186 0.037368 0.011571 0.070313 -0.091875 -0.092170 0.139323 0.001843 -0.195481 -0.071051 0.018516 0.087994 -0.088950 -0.006768 0.022944 -0.078728 0.019804 0.034330 -0.084489 0.066822 0.045952 0.039824 0.025499 -0.089017 -0.093305 0.008225 -0.178437 -0.045249 0.156472 -0.003366 0.031600 -0.112111 0.031002 0.017235 0.050212 0.068844 -0.052336 -0.091898 0.240929 -0.123371 -0.004094 0.099499 -0.095697 -0.192394 0.166625 -0.092753 -0.139534 -0.047868 0.042766 0.186704 0.021796 0.018163 -0.016969 0.065114 -0.043754 -0.051935 -0.024208 -0.049579 -0.030092 -0.069234 0.149659 -0.071942 0.045299 -0.067596 -0.091622 0.060503 -0.197020 -0.083051 0.139514 0.033104 0.107531
-22 -0.013324 -0.230635 0.030984 0.088591 0.098398 -0.146396 0.163462 0.062319 0.023642 -0.020648 -0.099384 0.080721 0.024131 -0.211143 0.037357 -0.247551 0.004881 -0.220844 -0.026082 0.005365 0.235157 -0.011633 0.157542 0.093512 0.056209 -0.167003 -0.100317 -0.023583 0.006637 0.191703 -0.011954 0.047766 -0.152447 -0.027922 -0.108449 -0.136209 0.200194 0.117877 -0.060827 -0.089997 -0.109545 0.076386 0.139793 0.174219 -0.123702 0.133703 0.092394 -0.075265 -0.038618 0.153266 -0.011535 0.186323 -0.021598 0.141207 -0.172176 -0.175036 -0.060644 0.081859 0.082517 -0.028496 0.111178 -0.097627 -0.040972 0.160566 -0.045475 -0.110720 -0.077981 0.050738 0.134783 -0.140888 -0.071112 0.053822 -0.108662 -0.030518 0.117828 -0.066935 0.077736 0.026950 0.008733 -0.083885 -0.111567 -0.043562 0.023202 -0.124658 -0.096351 0.084337 0.097076 0.012733 -0.073115 0.064600 0.044382 0.006441 0.085194 -0.057568 -0.141985 0.225707 -0.083323 -0.081665 0.116622 -0.032448 -0.156502 0.157640 -0.117900 -0.110662 -0.090711 0.023899 0.173482 0.005736 0.018212 -0.042824 0.065584 0.007001 -0.030511 0.008229 -0.081783 -0.010709 -0.054062 0.172114 -0.066413 -0.011092 -0.042839 -0.063262 0.123231 -0.230025 -0.185480 0.091342 0.055049 0.181622
-18 -0.015417 -0.214277 0.033568 0.083404 0.097872 -0.136150 0.154404 0.056222 0.014330 -0.011620 -0.096729 0.072457 0.026223 -0.203257 0.023977 -0.237472 0.008288 -0.197167 -0.026483 0.010702 0.221724 -0.008589 0.155733 0.087548 0.049251 -0.153534 -0.090079 -0.016234 0.008239 0.184684 -0.013617 0.052859 -0.144751 -0.016284 -0.107096 -0.126905 0.189289 0.107405 -0.049989 -0.084131 -0.112761 0.084732 0.130879 0.158727 -0.113563 0.124991 0.077743 -0.078021 -0.041654 0.141569 -0.004045 0.177146 -0.013704 0.141408 -0.172169 -0.155901 -0.059869 0.069247 0.066292 -0.015319 0.100727 -0.096667 -0.051426 0.146661 -0.030962 -0.120108 -0.069545 0.041643 0.121472 -0.126646 -0.059835 0.045643 -0.093986 -0.024604 0.099842 -0.067088 0.080691 0.030676 0.013505 -0.064096 -0.106151 -0.051701 0.018067 -0.132257 -0.087097 0.100177 0.077086 0.015532 -0.081299 0.060097 0.039849 0.012022 0.080452 -0.060635 -0.132832 0.221661 -0.089939 -0.063792 0.107578 -0.042526 -0.156943 0.152924 -0.109913 -0.109055 -0.076827 0.024759 0.166726 0.006231 0.015524 -0.042275 0.060000 -0.004546 -0.034933 0.000960 -0.073690 -0.017648 -0.052686 0.157787 -0.060782 -0.004237 -0.047121 -0.067493 0.103352 -0.211196 -0.156758 0.091279 0.046849 0.158989
-15 -0.038742 -0.181860 0.076422 0.131426 0.114511 -0.098082 0.178220 -0.010870 -0.073351 0.011362 -0.102118 0.022538 0.017705 -0.190122 -0.066676 -0.203583 0.031577 -0.098463 0.013143 0.065663 0.161812 -0.006469 0.180472 0.057628 -0.016408 -0.140837 -0.057834 -0.035330 -0.027581 0.201601 -0.016786 0.068714 -0.151775 0.025442 -0.151069 -0.079852 0.197741 0.085267 -0.030836 -0.120840 -0.152323 0.180546 0.119559 0.091588 -0.062273 0.098330 0.021480 -0.112452 -0.068518 0.119750 0.070742 0.224954 -0.001131 0.101519 -0.157838 -0.112446 -0.068340 0.037492 0.016034 0.044963 0.041362 -0.083752 -0.120886 0.138655 0.039096 -0.254089 -0.061011 0.000680 0.064335 -0.045800 0.042723 0.005416 -0.063940 0.055067 -0.029680 -0.099408 0.048734 0.063396 0.055471 0.101279 -0.075155 -0.121488 0.004649 -0.219147 -0.012046 0.203272 -0.072077 0.048097 -0.141964 0.013599 0.007170 0.079798 0.055884 -0.049460 -0.051760 0.258512 -0.143937 0.057159 0.090866 -0.145522 -0.209132 0.174501 -0.077477 -0.152952 -0.012560 0.055284 0.195636 0.030398 0.019197 -0.000547 0.072016 -0.072628 -0.064853 -0.043871 -0.032922 -0.043684 -0.075295 0.130494 -0.071538 0.086941 -0.077534 -0.114188 0.026202 -0.169823 -0.011775 0.173016 0.026535 0.057350
-21 -0.037420 -0.185872 0.075212 0.132879 0.113678 -0.109162 0.182616 -0.006157 -0.069429 0.015099 -0.106679 0.026768 0.019289 -0.203249 -0.063559 -0.210844 0.034637 -0.106484 0.014566 0.065066 0.167636 -0.008758 0.178195 0.061537 -0.008662 -0.145704 -0.056369 -0.033000 -0.026404 0.205464 -0.012827 0.065584 -0.158924 0.023812 -0.155812 -0.089556 0.206468 0.094386 -0.034654 -0.124362 -0.155135 0.181627 0.122971 0.098689 -0.074483 0.101350 0.031976 -0.111959 -0.065660 0.129340 0.071359 0.233446 0.000091 0.104223 -0.162610 -0.115789 -0.067045 0.041616 0.017170 0.040659 0.043462 -0.090928 -0.119716 0.149512 0.037119 -0.255346 -0.065734 0.000813 0.065450 -0.061481 0.036765 0.007069 -0.071992 0.055703 -0.015099 -0.099670 0.046978 0.058795 0.057399 0.096904 -0.081081 -0.120390 0.004858 -0.215712 -0.017138 0.200160 -0.064753 0.048738 -0.137154 0.013772 0.011125 0.075829 0.053434 -0.044530 -0.054077 0.259626 -0.147324 0.047909 0.094240 -0.146305 -0.214405 0.176736 -0.080233 -0.151088 -0.017518 0.060732 0.197316 0.025517 0.018441 -0.000798 0.078102 -0.066112 -0.064402 -0.035222 -0.035596 -0.042058 -0.081538 0.131786 -0.067695 0.083589 -0.080054 -0.110993 0.029966 -0.183058 -0.017387 0.173030 0.031126 0.062610
-17 -0.003820 -0.258748 0.007511 0.073799 0.087261 -0.166561 0.164332 0.103193 0.086301 -0.032979 -0.095967 0.114296 0.023835 -0.209231 0.087164 -0.264820 -0.005851 -0.289037 -0.053388 -0.027257 0.271622 -0.022929 0.143493 0.109600 0.104106 -0.188538 -0.123153 -0.017891 0.014887 0.181230 -0.006468 0.039014 -0.144970 -0.062753 -0.089660 -0.163364 0.210163 0.142884 -0.075443 -0.073802 -0.096341 0.035099 0.155266 0.212611 -0.160332 0.146925 0.138555 -0.055615 -0.024053 0.181979 -0.048630 0.178089 -0.024028 0.157596 -0.180789 -0.216782 -0.066465 0.103743 0.122200 -0.068508 0.147465 -0.110282 0.003898 0.167303 -0.087383 -0.037417 -0.086446 0.072099 0.169359 -0.193109 -0.129178 0.071290 -0.134761 -0.075162 0.187776 -0.051138 0.093057 0.001767 -0.021234 -0.181341 -0.128279 0.001945 0.036843 -0.066448 -0.146718 0.022767 0.183774 -0.000486 -0.038667 0.091760 0.056919 -0.029739 0.102607 -0.047634 -0.190349 0.213284 -0.046043 -0.152270 0.132497 0.031042 -0.125947 0.157345 -0.144651 -0.101663 -0.137798 0.000075 0.170512 -0.016977 0.015676 -0.058873 0.058126 0.048742 -0.019961 0.026767 -0.105519 0.007691 -0.039213 0.200967 -0.064217 -0.058417 -0.023250 -0.029369 0.180964 -0.262813 -0.285498 0.048213 0.068696 0.249874
-27 -0.032797 -0.195288 0.069216 0.133867 0.116239 -0.108610 0.185740 0.001955 -0.069550 0.011745 -0.109189 0.025742 0.016789 -0.195331 -0.063692 -0.214459 0.037900 -0.112968 0.012206 0.062131 0.171920 -0.008059 0.188124 0.063971 -0.005678 -0.152287 -0.063619 -0.034477 -0.029278 0.212291 -0.014158 0.063290 -0.162670 0.017202 -0.150986 -0.090824 0.207567 0.092173 -0.041477 -0.128388 -0.156671 0.178513 0.125848 0.103988 -0.072492 0.104946 0.038800 -0.114130 -0.067412 0.133543 0.070577 0.235690 -0.006427 0.108785 -0.161936 -0.123446 -0.068509 0.048673 0.026509 0.039035 0.043219 -0.087330 -0.122572 0.151673 0.033088 -0.258004 -0.068483 0.008963 0.071063 -0.058590 0.029735 0.015208 -0.073341 0.049656 -0.017644 -0.104967 0.052495 0.060227 0.055990 0.094341 -0.085367 -0.121196 0.006500 -0.214212 -0.020933 0.204051 -0.055618 0.050813 -0.148297 0.018130 0.005314 0.070086 0.059821 -0.045203 -0.059863 0.264393 -0.141945 0.042087 0.099247 -0.147625 -0.218392 0.180336 -0.078466 -0.159995 -0.019518 0.060393 0.201583 0.023242 0.017542 0.004161 0.076312 -0.067760 -0.062782 -0.038626 -0.033126 -0.048242 -0.077472 0.137043 -0.071898 0.078713 -0.083230 -0.106774 0.034610 -0.186021 -0.027404 0.176257 0.028692 0.068829
-13 -0.016251 -0.202210 0.044203 0.088466 0.096507 -0.126337 0.159309 0.039759 -0.004149 -0.007473 -0.097593 0.056936 0.024332 -0.200536 0.006949 -0.221349 0.011997 -0.176938 -0.019105 0.014213 0.206004 -0.006592 0.149878 0.080568 0.030047 -0.150166 -0.079204 -0.015752 0.001669 0.182771 -0.015711 0.054895 -0.149152 -0.011902 -0.116221 -0.118940 0.195229 0.105479 -0.047902 -0.094361 -0.116620 0.097874 0.131618 0.149932 -0.103497 0.118331 0.071413 -0.082989 -0.049628 0.140691 0.008150 0.189791 -0.012064 0.134192 -0.171061 -0.151393 -0.064362 0.064645 0.060668 -0.005979 0.090131 -0.090428 -0.066909 0.141823 -0.018341 -0.146798 -0.070384 0.033815 0.111008 -0.118907 -0.042392 0.039118 -0.091291 -0.008561 0.075513 -0.077024 0.069740 0.033468 0.017223 -0.035632 -0.096566 -0.069470 0.013172 -0.144055 -0.072647 0.118751 0.048097 0.022301 -0.091841 0.053665 0.037036 0.025523 0.069075 -0.060058 -0.112545 0.225052 -0.098872 -0.048194 0.099967 -0.061093 -0.164970 0.155288 -0.104344 -0.117836 -0.068449 0.027251 0.178255 0.012349 0.016290 -0.034276 0.068646 -0.016171 -0.038488 -0.000464 -0.068722 -0.024287 -0.060751 0.158546 -0.063426 0.011159 -0.052163 -0.070405 0.093148 -0.201658 -0.132882 0.112448 0.048381 0.147612
-16 -0.035511 -0.202433 0.074765 0.137003 0.120944 -0.118830 0.192556 0.003611 -0.063796 0.008194 -0.113915 0.035100 0.017987 -0.209628 -0.054792 -0.230371 0.031344 -0.130028 0.005399 0.060351 0.183567 -0.007812 0.195586 0.067352 -0.000746 -0.156934 -0.066758 -0.034735 -0.023160 0.217018 -0.017857 0.071274 -0.169265 0.016625 -0.155325 -0.095593 0.218590 0.094721 -0.042875 -0.128845 -0.157651 0.177348 0.135764 0.114344 -0.078313 0.107115 0.041332 -0.114620 -0.072006 0.140880 0.065547 0.235994 -0.004975 0.119182 -0.171357 -0.127930 -0.069664 0.051912 0.027303 0.033553 0.053918 -0.097968 -0.121946 0.152773 0.029148 -0.257393 -0.069424 0.013343 0.078061 -0.073751 0.027466 0.012917 -0.077428 0.046133 -0.000511 -0.104481 0.061024 0.061056 0.054253 0.080574 -0.086068 -0.123117 0.007125 -0.222340 -0.023653 0.207997 -0.047278 0.051753 -0.143597 0.023012 0.008929 0.073471 0.060931 -0.052019 -0.070416 0.277388 -0.146257 0.033883 0.101535 -0.145533 -0.219929 0.189848 -0.087398 -0.162705 -0.030509 0.055889 0.206549 0.028086 0.023714 -0.004239 0.078097 -0.061218 -0.065298 -0.034074 -0.040147 -0.044006 -0.080637 0.149864 -0.073541 0.072015 -0.083348 -0.112196 0.039528 -0.199341 -0.039605 0.177897 0.033195 0.086814
-10 -0.027585 -0.194088 0.059197 0.119915 0.110972 -0.109276 0.174774 0.014288 -0.042095 -0.000858 -0.100241 0.042340 0.015622 -0.194950 -0.039811 -0.211997 0.029892 -0.130081 -0.000935 0.049677 0.180622 -0.008247 0.175306 0.071121 0.006256 -0.149930 -0.067700 -0.032686 -0.017300 0.199301 -0.015064 0.058100 -0.155358 0.013960 -0.142041 -0.098388 0.204942 0.091471 -0.045583 -0.114575 -0.142506 0.146433 0.133451 0.124666 -0.082104 0.110766 0.052044 -0.105706 -0.057250 0.130125 0.046488 0.215138 -0.005023 0.115444 -0.165828 -0.127673 -0.063402 0.051871 0.031160 0.026004 0.060362 -0.090928 -0.104118 0.150557 0.012237 -0.215296 -0.068170 0.014316 0.081505 -0.084544 0.007917 0.021841 -0.077922 0.032457 0.022610 -0.088440 0.057599 0.052640 0.040163 0.045724 -0.092811 -0.102962 0.008316 -0.186414 -0.038811 0.175640 -0.023215 0.038689 -0.124989 0.026973 0.020717 0.056677 0.068148 -0.047932 -0.078206 0.253664 -0.129277 0.009820 0.098881 -0.111326 -0.200409 0.169981 -0.092098 -0.142026 -0.041227 0.047309 0.194378 0.028011 0.015665 -0.009721 0.075129 -0.051944 -0.050750 -0.027879 -0.043975 -0.038653 -0.069770 0.145200 -0.069432 0.052571 -0.072166 -0.100100 0.053520 -0.189645 -0.060939 0.148898 0.033137 0.097645
-19 -0.027732 -0.191410 0.071374 0.126421 0.111070 -0.107876 0.176676 0.002585 -0.056623 0.007753 -0.107521 0.034261 0.017425 -0.196124 -0.054218 -0.209829 0.028584 -0.119391 0.010084 0.061913 0.173410 -0.011680 0.179995 0.065147 -0.001625 -0.142977 -0.063433 -0.032499 -0.024087 0.202601 -0.014654 0.065021 -0.157166 0.014068 -0.147521 -0.090713 0.201155 0.093299 -0.036933 -0.116991 -0.146786 0.164469 0.126893 0.108293 -0.072268 0.104702 0.041552 -0.111925 -0.064965 0.130148 0.057931 0.221071 -0.006557 0.107943 -0.158583 -0.118838 -0.068929 0.049770 0.029246 0.031526 0.049580 -0.091647 -0.108593 0.149711 0.025498 -0.241462 -0.062069 0.007796 0.072132 -0.069099 0.023806 0.015895 -0.073660 0.045420 -0.004704 -0.098964 0.055777 0.060156 0.047455 0.072451 -0.083713 -0.113153 0.005848 -0.206370 -0.026630 0.191575 -0.042831 0.048624 -0.137622 0.019955 0.012079 0.069761 0.063439 -0.046717 -0.067139 0.259465 -0.137931 0.031655 0.095115 -0.133943 -0.204654 0.176064 -0.087317 -0.151846 -0.029655 0.053095 0.194311 0.026293 0.017557 -0.005392 0.070930 -0.060431 -0.056958 -0.035492 -0.035889 -0.039722 -0.070587 0.140291 -0.072385 0.073148 -0.077481 -0.105852 0.037980 -0.184668 -0.039701 0.162597 0.027359 0.074613
-23 -0.033210 -0.184668 0.068293 0.134566 0.114270 -0.104607 0.178711 0.001390 -0.065522 0.009054 -0.106138 0.027556 0.018583 -0.199027 -0.063989 -0.206200 0.033015 -0.105313 0.014056 0.067949 0.166519 -0.009827 0.178455 0.063617 -0.006858 -0.146158 -0.062987 -0.037576 -0.029466 0.209047 -0.015356 0.066887 -0.160449 0.021807 -0.155659 -0.090378 0.207961 0.088324 -0.040872 -0.123138 -0.151890 0.176836 0.126912 0.103274 -0.069964 0.100173 0.030907 -0.109336 -0.069626 0.127748 0.067974 0.227998 -0.000047 0.103285 -0.161713 -0.120993 -0.068281 0.045139 0.020741 0.034411 0.045739 -0.092077 -0.115655 0.148847 0.035084 -0.252089 -0.062115 0.008133 0.069460 -0.054618 0.033052 0.012278 -0.067598 0.053203 -0.019130 -0.097868 0.052464 0.061815 0.059491 0.093377 -0.076428 -0.121617 0.008539 -0.212696 -0.018011 0.203398 -0.061353 0.051035 -0.142295 0.014292 0.010089 0.074233 0.053472 -0.047926 -0.059945 0.256730 -0.142284 0.041962 0.092629 -0.139135 -0.213449 0.175437 -0.076828 -0.156503 -0.021848 0.053856 0.192573 0.025691 0.021557 0.001070 0.069397 -0.069436 -0.063409 -0.040655 -0.036039 -0.045850 -0.079422 0.133649 -0.068391 0.078443 -0.077553 -0.106411 0.028381 -0.181549 -0.019292 0.166210 0.027056 0.064981
-12 -0.021273 -0.227668 0.038558 0.090407 0.099022 -0.140312 0.164310 0.052033 0.009098 -0.013410 -0.097127 0.077289 0.022158 -0.207640 0.021235 -0.240299 0.006905 -0.202974 -0.020711 0.006990 0.226572 -0.006636 0.162136 0.086941 0.045041 -0.167648 -0.092316 -0.020622 0.001998 0.192480 -0.011041 0.053884 -0.157959 -0.018872 -0.112976 -0.125813 0.202160 0.115976 -0.054787 -0.090532 -0.118860 0.091723 0.145099 0.166208 -0.120442 0.130763 0.087454 -0.084446 -0.048270 0.149799 -0.000930 0.194755 -0.016566 0.135677 -0.178469 -0.171928 -0.064194 0.077802 0.071331 -0.014707 0.104369 -0.097693 -0.055641 0.159833 -0.037280 -0.129457 -0.070626 0.042838 0.127276 -0.129173 -0.052652 0.049975 -0.101368 -0.018337 0.098129 -0.073238 0.081095 0.033745 0.013358 -0.056144 -0.107931 -0.054709 0.014202 -0.137441 -0.089349 0.108359 0.072167 0.020375 -0.085731 0.056013 0.039300 0.019503 0.078827 -0.056725 -0.129791 0.227478 -0.089512 -0.069089 0.113283 -0.044508 -0.168694 0.157203 -0.113424 -0.119602 -0.082808 0.021959 0.180182 0.008560 0.020195 -0.041412 0.060062 -0.004268 -0.036427 0.002955 -0.074725 -0.013607 -0.056984 0.172375 -0.070957 -0.001239 -0.051512 -0.070750 0.111761 -0.219853 -0.165831 0.102570 0.048231 0.167540
Index: graph/ratings_small.edge
===================================================================
--- graph/ratings_small.edge	(date 1558009865000)
+++ graph/ratings_small.edge	(date 1558009865000)
@@ -1,1000 +0,0 @@
-1 1193 5
-1 661 3
-1 914 3
-1 3408 4
-1 2355 5
-1 1197 3
-1 1287 5
-1 2804 5
-1 594 4
-1 595 5
-1 938 4
-1 2398 4
-1 2918 4
-1 1035 5
-1 2791 4
-1 2687 3
-1 2018 4
-1 3105 5
-1 2797 4
-1 2321 3
-1 720 3
-1 1270 5
-1 527 5
-1 2340 3
-1 1721 4
-1 1545 4
-1 745 3
-1 2294 4
-1 3186 4
-1 588 4
-1 1907 4
-1 783 4
-1 1836 5
-1 1022 5
-1 2762 4
-1 150 5
-1 1 5
-1 1961 5
-1 1962 4
-1 2692 4
-1 260 4
-1 1028 5
-1 1029 5
-1 1207 4
-1 2028 5
-1 531 4
-1 3114 4
-1 608 4
-1 1246 4
-2 1357 5
-2 3068 4
-2 1537 4
-2 647 3
-2 2194 4
-2 648 4
-2 2268 5
-2 2628 3
-2 1103 3
-2 2916 3
-2 3468 5
-2 1792 3
-2 1687 3
-2 1213 2
-2 3578 5
-2 2881 3
-2 3030 4
-2 1217 3
-2 3105 4
-2 434 2
-2 3107 2
-2 3108 3
-2 3035 4
-2 1253 3
-2 1610 5
-2 292 3
-2 2236 5
-2 3071 4
-2 902 2
-2 368 4
-2 3147 5
-2 1544 4
-2 1293 5
-2 1188 4
-2 3255 4
-2 3256 2
-2 3257 3
-2 110 5
-2 2278 3
-2 2490 3
-2 1834 4
-2 3471 5
-2 589 4
-2 1690 3
-2 3654 3
-2 2852 3
-2 1945 5
-2 982 4
-2 1873 4
-2 2858 4
-2 1225 5
-2 2028 4
-2 515 5
-2 442 3
-2 2312 3
-2 265 4
-2 1084 3
-2 480 5
-2 1442 4
-2 2067 5
-2 1265 3
-2 1370 5
-2 1193 5
-2 1801 3
-2 1372 3
-2 2353 4
-2 2427 2
-2 1552 3
-2 736 4
-2 1198 4
-2 593 5
-2 2359 3
-2 95 2
-2 2717 3
-2 2571 4
-2 1917 3
-2 2396 4
-2 3735 3
-2 1953 4
-2 3809 3
-2 1954 5
-2 1955 4
-2 235 3
-2 1124 5
-2 1957 5
-2 163 4
-2 21 1
-2 165 3
-2 2321 3
-2 1090 2
-2 380 5
-2 2501 5
-2 349 4
-2 457 4
-2 1096 4
-2 920 5
-2 459 3
-2 1527 4
-2 3418 4
-2 1385 3
-2 3451 4
-2 3095 4
-2 780 3
-2 498 3
-2 2728 3
-2 2002 5
-2 1962 5
-2 1784 5
-2 2943 4
-2 2006 3
-2 318 5
-2 1207 4
-2 1968 2
-2 3678 3
-2 1244 3
-2 1245 2
-2 1246 5
-2 3893 1
-2 1247 5
-3 3421 4
-3 1641 2
-3 648 3
-3 1394 4
-3 3534 3
-3 104 4
-3 2735 4
-3 1431 3
-3 3868 3
-3 1079 5
-3 2997 3
-3 1615 5
-3 1291 4
-3 653 4
-3 2167 5
-3 3619 2
-3 260 5
-3 2858 4
-3 3114 3
-3 1049 4
-3 552 4
-3 480 4
-3 1265 2
-3 1266 5
-3 733 5
-3 2355 5
-3 1197 5
-3 1198 5
-3 1378 5
-3 593 3
-3 1379 4
-3 3552 5
-3 1304 5
-3 1270 3
-3 2470 4
-3 3168 4
-3 1961 4
-3 3671 5
-3 2006 4
-3 2871 4
-3 2115 4
-3 1968 4
-3 1136 5
-3 2081 4
-4 3468 5
-4 2951 4
-4 1214 4
-4 1036 4
-4 260 5
-4 2028 5
-4 480 4
-4 1198 5
-4 1954 5
-4 3418 4
-4 3702 4
-4 2366 4
-4 1387 5
-4 3527 1
-4 1201 5
-4 2692 5
-4 2947 5
-4 1240 5
-5 2333 4
-5 1175 5
-5 39 3
-5 288 2
-5 2337 5
-5 1535 4
-5 1392 4
-5 2268 2
-5 1466 3
-5 860 2
-5 1683 3
-5 866 4
-5 1684 3
-5 2916 1
-5 2770 4
-5 215 3
-5 1759 4
-5 501 1
-5 3578 2
-5 506 4
-5 1250 5
-5 3793 2
-5 509 4
-5 41 4
-5 1610 4
-5 2058 1
-5 3799 3
-5 2997 5
-5 47 3
-5 2700 4
-5 296 4
-5 581 3
-5 1617 3
-5 728 4
-5 299 3
-5 3079 2
-5 2560 4
-5 150 2
-5 224 3
-5 3728 2
-5 229 3
-5 6 2
-5 3006 3
-5 2858 4
-5 1046 5
-5 515 4
-5 800 2
-5 50 5
-5 52 2
-5 1191 2
-5 1192 5
-5 733 1
-5 3081 3
-5 377 4
-5 2353 3
-5 1268 2
-5 3083 5
-5 2427 5
-5 3513 1
-5 2428 3
-5 2355 5
-5 2282 3
-5 3514 2
-5 1554 3
-5 1912 3
-5 593 4
-5 2359 3
-5 2716 3
-5 1485 3
-5 2717 1
-5 2571 5
-5 2289 4
-5 162 4
-5 1127 1
-5 3016 4
-5 2070 2
-5 1704 3
-5 3163 5
-5 2437 2
-5 2291 5
-5 1635 4
-5 1279 1
-5 2721 3
-5 2723 4
-5 1921 4
-5 2725 2
-5 1923 3
-5 2580 4
-5 3386 2
-5 3744 1
-5 968 3
-5 896 4
-5 3020 1
-5 1788 3
-5 318 3
-5 176 4
-5 461 3
-5 608 4
-5 1429 3
-5 2159 1
-5 1715 4
-5 1643 3
-5 3249 1
-5 3176 2
-5 1719 3
-5 2806 2
-5 2734 2
-5 1649 4
-5 321 3
-5 2013 3
-5 3100 1
-5 2952 2
-5 1213 5
-5 1794 3
-5 2599 5
-5 1500 3
-5 3105 2
-5 2959 4
-5 1509 3
-5 1721 1
-5 1722 2
-5 1650 3
-5 908 4
-5 1653 2
-5 2384 3
-5 1729 4
-5 3476 3
-5 2890 4
-5 3113 1
-5 2028 2
-5 16 3
-5 265 3
-5 2029 4
-5 194 3
-5 551 4
-5 1513 4
-5 3046 3
-5 2318 4
-5 1517 4
-5 1089 5
-5 3260 4
-5 913 5
-5 1730 4
-5 3408 3
-5 3409 3
-5 2607 2
-5 1449 4
-5 1733 3
-5 2390 4
-5 1734 4
-5 3266 2
-5 3267 4
-5 3624 3
-5 2395 5
-5 1594 1
-5 2683 3
-5 412 2
-5 2759 3
-5 994 5
-5 1884 3
-5 1885 4
-5 272 3
-5 24 1
-5 3051 2
-5 348 4
-5 2323 4
-5 1093 2
-5 29 5
-5 562 4
-5 1095 4
-5 1527 3
-5 1529 4
-5 3418 3
-5 2188 1
-5 497 3
-5 202 2
-5 1747 2
-5 2908 4
-5 2762 3
-5 2692 4
-5 1966 2
-5 3499 3
-5 353 2
-5 32 4
-5 1243 3
-5 1897 4
-5 1171 4
-5 3786 3
-5 34 4
-5 357 2
-5 36 3
-5 714 4
-6 2406 5
-6 1101 4
-6 3717 4
-6 1030 4
-6 1688 5
-6 1035 5
-6 3578 4
-6 364 4
-6 3501 5
-6 3072 4
-6 368 4
-6 296 2
-6 3074 5
-6 1188 4
-6 3508 3
-6 588 4
-6 1 4
-6 1043 4
-6 2858 1
-6 377 3
-6 595 4
-6 597 5
-6 383 4
-6 2506 3
-6 3524 3
-6 1569 4
-6 2006 4
-6 2081 4
-6 2082 3
-6 3600 3
-6 3604 5
-6 2802 4
-6 3534 4
-6 3536 5
-6 3753 5
-6 3682 3
-6 3685 3
-6 3610 3
-6 1296 3
-6 838 4
-6 1007 3
-6 1947 5
-6 2966 5
-6 17 4
-6 1441 4
-6 1088 5
-6 912 4
-6 199 5
-6 914 5
-6 3408 5
-6 1806 3
-6 3624 4
-6 2469 3
-6 2396 4
-6 1959 3
-6 2321 3
-6 1380 5
-6 920 4
-6 569 4
-6 1674 4
-6 3565 4
-6 1028 4
-6 34 4
-7 648 4
-7 861 4
-7 2916 5
-7 3578 3
-7 3793 3
-7 1610 5
-7 589 5
-7 6 4
-7 442 4
-7 733 5
-7 377 3
-7 2353 5
-7 2571 5
-7 380 5
-7 1997 5
-7 1270 4
-7 457 5
-7 1573 4
-7 3753 4
-7 3107 3
-7 474 5
-7 1722 4
-7 3256 5
-7 110 5
-7 1221 4
-7 2028 5
-7 480 4
-7 349 5
-7 3418 3
-8 39 3
-8 2336 3
-8 288 5
-8 3425 3
-8 2268 3
-8 1466 4
-8 1393 5
-8 1682 4
-8 2916 5
-8 506 3
-8 508 3
-8 3213 3
-8 42 3
-8 650 5
-8 3500 3
-8 296 5
-8 3147 5
-8 3148 3
-8 2702 3
-8 2278 3
-8 1476 3
-8 2490 2
-8 589 5
-8 1836 4
-8 1693 3
-8 150 4
-8 151 4
-8 1 4
-8 510 3
-8 4 3
-8 3006 3
-8 2858 5
-8 1621 3
-8 1265 5
-8 733 3
-8 377 4
-8 3155 3
-8 2427 5
-8 58 5
-8 2712 3
-8 2429 2
-8 1840 4
-8 2571 5
-8 1916 5
-8 230 4
-8 1120 4
-8 161 3
-8 163 5
-8 1411 5
-8 524 5
-8 1059 3
-8 527 4
-8 454 3
-8 1701 4
-8 741 5
-8 1704 5
-8 1277 3
-8 2291 5
-8 1639 5
-8 3528 4
-8 2297 3
-8 3386 3
-8 2006 3
-8 608 5
-8 465 5
-8 538 3
-8 393 2
-8 2442 4
-8 1357 4
-8 73 4
-8 3246 4
-8 3173 2
-8 1573 4
-8 105 4
-8 1213 5
-8 253 5
-8 3105 4
-8 3107 5
-8 3250 3
-8 3252 3
-8 1721 5
-8 476 3
-8 3256 5
-8 908 5
-8 3257 3
-8 1653 5
-8 110 5
-8 111 5
-8 3259 4
-8 3186 4
-8 1589 4
-8 2023 3
-8 14 4
-8 2028 5
-8 337 5
-8 265 4
-8 16 4
-8 17 4
-8 2314 3
-8 2600 2
-8 480 5
-8 269 4
-8 555 4
-8 1801 3
-8 3260 3
-8 1730 4
-8 1660 3
-8 3265 5
-8 1735 4
-8 3267 5
-8 3481 4
-8 2396 5
-8 2686 4
-8 2688 3
-8 345 3
-8 2320 2
-8 24 4
-8 25 5
-8 2324 3
-8 349 4
-8 562 5
-8 2329 5
-8 1810 2
-8 2541 3
-8 3418 3
-8 1673 5
-8 2908 3
-8 1678 5
-8 2692 5
-8 1027 4
-8 282 3
-8 36 4
-9 2268 4
-9 1466 4
-9 1393 3
-9 861 2
-9 1682 4
-9 3717 3
-9 508 3
-9 3793 4
-9 720 4
-9 47 5
-9 3147 4
-9 3148 3
-9 1617 4
-9 2278 4
-9 223 4
-9 150 3
-9 3298 4
-9 1 5
-9 3006 3
-9 2858 4
-9 3948 3
-9 50 4
-9 1265 4
-9 805 3
-9 3510 3
-9 377 3
-9 1552 2
-9 3513 3
-9 2355 4
-9 1912 3
-9 593 5
-9 2571 5
-9 597 3
-9 300 4
-9 1777 4
-9 162 4
-9 524 4
-9 3301 2
-9 1343 3
-9 527 5
-9 3160 3
-9 529 5
-9 457 5
-9 1704 4
-9 745 4
-9 3452 2
-9 2294 4
-9 1921 4
-9 1639 4
-9 1923 5
-9 1784 3
-9 1060 4
-9 318 5
-9 608 4
-9 1356 3
-9 1358 4
-9 3178 3
-9 3751 4
-9 3826 2
-9 1213 4
-9 3755 2
-9 2599 2
-9 2302 4
-9 1500 4
-9 2959 4
-9 1148 4
-9 2166 4
-9 1721 5
-9 3253 4
-9 3255 4
-9 1653 4
-9 838 3
-9 1584 5
-9 1221 4
-9 1223 4
-9 2890 5
-9 2028 5
-9 3114 4
-9 16 4
-9 480 4
-9 1089 4
-9 912 4
-9 1446 3
-9 3408 4
-9 3623 4
-9 778 5
-9 1669 3
-9 3484 2
-9 412 3
-9 3916 3
-9 994 4
-9 1233 3
-9 1307 4
-9 25 4
-9 2324 5
-9 349 4
-9 920 3
-9 3270 3
-9 2762 4
-9 1961 5
-9 2692 4
-9 1310 3
-9 428 3
-10 2622 5
-10 648 4
-10 2628 3
-10 3358 5
-10 3359 3
-10 1682 5
-10 1756 4
-10 1320 3
-10 2124 3
-10 2125 5
-10 1250 3
-10 1252 3
-10 1253 5
-10 720 5
-10 3868 3
-10 1254 3
-10 3869 3
-10 1256 3
-10 3500 5
-10 1257 5
-10 3501 5
-10 2997 4
-10 653 4
-10 1831 5
-10 3363 5
-10 586 3
-10 587 5
-10 3438 3
-10 588 4
-10 3439 2
-10 589 4
-10 1690 4
-10 3296 4
-10 223 2
-10 150 5
-10 2496 4
-10 1 5
-10 2497 4
-10 2 5
-10 2498 4
-10 153 3
-10 7 4
-10 2133 4
-10 3948 4
-10 2136 4
-10 2137 4
-10 802 5
-10 1409 5
-10 2067 3
-10 1265 5
-10 1339 5
-10 1197 5
-10 1198 5
-10 2640 5
-10 594 5
-10 2716 4
-10 595 5
-10 2717 4
-10 2571 5
-10 596 4
-10 3447 5
-10 597 4
-10 1918 3
-10 1411 4
-10 2072 4
-10 1270 4
-10 1271 5
-10 1345 3
-10 2077 3
-10 2078 4
-10 1276 3
-10 743 3
-10 671 3
-10 1278 5
-10 745 5
-10 3451 5
-10 3525 2
-10 1921 4
-10 1923 5
-10 1927 3
-10 3386 4
-10 3388 3
-10 1784 5
-10 316 5
-10 317 4
-10 318 4
-10 248 5
-10 2080 4
-10 2081 5
-10 2082 3
-10 1282 5
-10 1356 5
-10 1283 3
-10 1357 5
-10 1286 5
-10 1287 3
-10 2804 5
-10 3608 3
-10 2662 4
-10 3466 5
-10 3100 3
-10 2300 5
-10 253 5
-10 180 2
-10 2599 5
-10 2302 5
-10 329 5
-10 3034 4
-10 3108 5
-10 3035 3
-10 186 3
-10 2161 5
-10 3037 5
-10 2090 4
-10 3039 4
-10 902 5
-10 830 5
-10 2093 4
-10 1291 5
-10 904 4
-10 2094 4
-10 1292 5
-10 1293 4
-10 2096 5
-10 1294 4
-10 765 4
-10 3471 5
-10 1009 5
-10 1947 5
-10 1948 4
-10 333 3
-10 260 5
-10 3114 4
-10 2312 5
-10 339 5
-10 1513 3
-10 2316 4
-10 1441 5
-10 1517 3
-10 1371 4
-10 2174 5
-10 1372 4
-10 912 3
-10 913 3
-10 1374 4
-10 914 5
-10 1375 4
-10 915 5
-10 1376 4
-10 1377 3
-10 916 5
-10 918 5
-10 1012 3
-10 3481 4
-10 3629 3
-10 1015 3
-10 1016 5
-10 1954 3
-10 1019 4
-10 1884 2
-10 1959 5
-10 344 4
-10 24 3
-10 2321 4
-10 275 4
-10 2324 5
-10 2252 5
-10 277 3
-10 1380 5
-10 920 5
-10 923 3
-10 3701 3
-10 3702 3
-10 780 5
-10 3703 2
-10 1387 3
-10 926 4
-10 3704 2
-10 1020 3
-10 784 3
-10 858 3
-10 1022 5
-10 2762 5
-10 1023 5
-10 1961 5
-10 1025 4
-10 2693 5
-10 1028 5
-10 1967 5
-10 351 4
-10 32 5
-10 282 5
-10 357 5
-10 2405 4
-10 2407 5
-10 2336 5
-10 3066 2
-10 2193 5
-10 932 2
-10 1394 3
-10 938 4
-10 1101 4
-10 1030 3
-10 2770 4
-10 1104 4
-10 1032 4
-10 2918 5
-10 1035 5
-10 435 4
-10 364 4
-10 292 4
-10 3072 4
-10 368 4
-10 1544 4
-10 1617 3
-10 943 4
-10 3723 4
-10 1042 5
-10 2858 3
-10 370 3
-10 2423 5
-10 3153 2
-10 2424 5
-10 3155 5
-10 2355 4
-10 3086 4
-10 3087 5
-10 953 5
-10 954 5
-10 3809 5
-10 3591 5
-10 2863 4
-10 1124 5
-10 3593 4
-10 2791 4
-10 1125 4
-10 520 4
-10 3668 4
-10 1127 4
-10 2795 3
-10 1129 4
-10 2797 5
-10 1059 5
-10 2501 5
-10 527 4
-10 2431 5
-10 62 5
-10 1704 5
-10 3309 3
-10 1633 4
-10 2436 5
Index: graph/ratings_val.edge
===================================================================
--- graph/ratings_val.edge	(date 1558009865000)
+++ graph/ratings_val.edge	(date 1558009865000)
@@ -1,1300 +0,0 @@
-81 69 2
-81 3811 4
-81 3741 5
-81 3742 5
-81 1201 5
-81 1207 5
-81 2879 4
-81 2010 5
-81 1213 5
-81 1214 5
-81 1218 5
-81 1221 5
-81 3836 5
-81 1224 5
-81 2028 5
-81 1228 5
-81 1084 5
-81 482 1
-81 1300 5
-81 1230 4
-81 1304 5
-81 2971 4
-81 1233 5
-81 1234 5
-81 1099 4
-81 1247 5
-82 718 4
-82 648 4
-82 1320 4
-82 3863 3
-82 3793 3
-82 1183 4
-82 587 5
-82 589 5
-82 1407 4
-82 802 5
-82 805 4
-82 733 4
-82 1197 4
-82 593 5
-82 1411 4
-82 1343 4
-82 1270 5
-82 748 3
-82 1358 4
-82 3100 5
-82 3105 5
-82 3108 4
-82 832 4
-82 3044 4
-82 1370 4
-82 1372 4
-82 1459 3
-82 780 5
-82 2268 4
-82 1393 5
-82 1610 5
-82 3072 4
-82 3147 4
-82 1617 4
-82 2278 4
-82 2424 4
-82 2353 5
-82 1625 4
-82 880 3
-82 2501 5
-82 1704 5
-82 3316 4
-82 1644 3
-82 3176 4
-82 1573 3
-82 3252 4
-82 1721 5
-82 1722 4
-82 3256 5
-82 110 5
-82 3185 4
-82 1584 5
-82 1801 2
-82 3273 3
-82 1747 4
-82 1762 4
-82 150 5
-82 3510 5
-82 3513 4
-82 2571 5
-82 230 4
-82 161 5
-82 2722 4
-82 1784 5
-82 318 5
-82 256 4
-82 185 3
-82 3476 4
-82 10 4
-82 16 4
-82 21 3
-82 24 3
-82 349 4
-82 2762 5
-82 1892 4
-82 350 3
-82 1968 3
-82 32 4
-82 2916 4
-82 3578 4
-82 431 4
-82 508 3
-82 47 4
-82 296 5
-82 2858 4
-82 50 3
-82 55 3
-82 377 3
-82 379 3
-82 3662 3
-82 2791 3
-82 451 3
-82 527 4
-82 62 5
-82 457 5
-82 608 4
-82 3821 2
-82 3753 5
-82 3755 5
-82 3686 4
-82 474 4
-82 3697 3
-82 480 4
-82 628 4
-82 1307 5
-82 1092 5
-83 2995 2
-83 3798 4
-83 653 3
-83 2 3
-83 3877 1
-83 2143 5
-83 1270 5
-83 2161 5
-83 2162 3
-83 2093 2
-83 3114 3
-83 1513 3
-83 2174 4
-83 913 4
-83 2336 5
-83 2193 5
-83 2424 4
-83 3081 2
-83 3155 5
-83 2355 4
-83 3159 5
-83 2502 5
-83 1702 3
-83 2369 2
-83 968 3
-83 3174 4
-83 3175 5
-83 2519 1
-83 3176 2
-83 1573 4
-83 2453 4
-83 1654 2
-83 3408 5
-83 2390 5
-83 2394 3
-83 2396 5
-83 2541 4
-83 2546 3
-83 2622 5
-83 2628 5
-83 2629 5
-83 2700 2
-83 223 3
-83 2710 4
-83 2712 1
-83 2716 5
-83 2723 1
-83 2724 5
-83 2581 1
-83 317 1
-83 3534 3
-83 2734 4
-83 3479 5
-83 260 5
-83 3555 4
-83 2683 2
-83 2688 3
-83 1959 4
-83 276 2
-83 2762 5
-83 2763 5
-83 2690 3
-83 2692 5
-83 2694 1
-83 1967 5
-83 3578 4
-83 2858 2
-83 2861 5
-83 1127 4
-83 2797 5
-83 60 5
-83 3744 3
-83 2005 4
-83 2948 5
-83 2013 2
-83 3755 3
-83 2881 3
-83 1073 4
-83 2959 5
-83 2961 5
-83 2968 4
-83 2105 5
-83 2975 4
-84 1197 5
-84 1198 5
-84 1199 1
-84 1291 5
-84 1394 1
-84 941 2
-84 2502 5
-84 110 5
-84 2628 1
-84 2571 5
-84 1799 5
-84 260 5
-84 3578 5
-84 435 2
-84 2858 4
-84 527 4
-84 457 4
-84 1214 5
-84 1222 5
-84 2028 4
-84 480 4
-84 1240 4
-84 2985 3
-85 648 3
-85 2125 3
-85 3793 3
-85 3005 5
-85 3949 3
-85 1282 1
-85 2188 3
-85 879 1
-85 3238 1
-85 3317 2
-85 3324 5
-85 3325 2
-85 3408 5
-85 3409 5
-85 3273 1
-85 2706 2
-85 3298 4
-85 3299 2
-85 3510 4
-85 2710 2
-85 3617 4
-85 2683 4
-85 277 3
-85 2908 3
-85 1101 2
-85 3578 3
-85 47 2
-85 2858 5
-85 3809 2
-85 1997 5
-85 3821 2
-85 3825 3
-85 3826 4
-85 3755 4
-85 2028 2
-85 3914 4
-85 3785 5
-85 3859 3
-86 648 5
-86 2997 1
-86 3005 5
-86 1344 5
-86 754 5
-86 2236 5
-86 903 5
-86 904 5
-86 3118 3
-86 920 5
-86 2184 4
-86 3147 5
-86 1544 4
-86 3175 4
-86 1721 5
-86 2454 5
-86 1582 5
-86 2396 5
-86 2628 5
-86 2485 5
-86 1688 3
-86 1760 4
-86 1907 5
-86 153 4
-86 158 4
-86 2716 3
-86 236 5
-86 260 5
-86 3623 4
-86 1013 4
-86 1015 4
-86 2762 5
-86 1035 3
-86 364 4
-86 54 4
-86 2005 4
-86 2006 4
-86 609 4
-86 552 5
-86 480 4
-86 1088 5
-87 3793 4
-87 583 1
-87 585 1
-87 589 5
-87 733 5
-87 1197 3
-87 1198 4
-87 1277 4
-87 2157 1
-87 826 1
-87 1291 4
-87 1387 5
-87 3062 1
-87 2194 5
-87 1610 5
-87 2347 1
-87 2366 1
-87 3178 5
-87 1649 1
-87 110 5
-87 2529 1
-87 2533 1
-87 3418 1
-87 145 5
-87 1912 1
-87 2571 5
-87 1916 1
-87 3384 1
-87 1785 1
-87 260 5
-87 3624 1
-87 3555 5
-87 1954 5
-87 21 1
-87 3633 1
-87 3639 5
-87 1032 1
-87 1036 3
-87 3578 5
-87 438 1
-87 1112 1
-87 377 1
-87 457 5
-87 2000 5
-87 1201 4
-87 3672 1
-87 2006 5
-87 2947 5
-87 2948 1
-87 2949 1
-87 390 1
-87 2028 4
-87 480 5
-87 555 1
-87 1240 4
-88 1250 4
-88 2997 2
-88 1265 1
-88 1193 5
-88 1198 4
-88 1284 4
-88 2302 4
-88 1293 4
-88 908 5
-88 1299 5
-88 912 5
-88 1446 5
-88 2321 4
-88 2324 5
-88 923 5
-88 858 5
-88 3067 4
-88 3068 4
-88 1394 1
-88 3210 4
-88 3072 4
-88 954 5
-88 1704 5
-88 110 4
-88 111 5
-88 1594 2
-88 1747 4
-88 1674 4
-88 3359 4
-88 3361 4
-88 2710 1
-88 1923 5
-88 318 4
-88 1945 5
-88 11 4
-88 1956 5
-88 1957 5
-88 1961 5
-88 1962 4
-88 1968 4
-88 2915 5
-88 2927 5
-88 3730 2
-88 527 5
-88 1203 5
-88 1207 5
-88 1208 3
-88 608 5
-88 1079 3
-88 1221 5
-88 1225 5
-88 2028 5
-88 1228 4
-88 1084 5
-88 480 3
-88 1300 5
-88 1230 5
-88 1231 5
-88 2973 3
-88 1307 4
-88 1090 3
-88 1244 3
-88 1172 5
-88 3788 5
-89 3868 1
-89 3869 1
-89 1193 5
-89 733 5
-89 920 5
-89 2431 5
-89 1704 4
-89 3173 4
-89 3408 4
-89 3265 4
-89 1961 4
-89 370 1
-89 2791 1
-89 2792 1
-89 527 5
-89 3740 3
-89 2947 5
-89 1136 1
-89 1218 4
-89 2028 4
-89 1080 1
-90 647 3
-90 648 3
-90 3863 4
-90 1250 4
-90 3869 5
-90 3798 4
-90 2997 4
-90 586 3
-90 1 3
-90 3005 3
-90 1401 3
-90 2060 2
-90 3948 4
-90 802 2
-90 3879 3
-90 1265 3
-90 1266 5
-90 736 3
-90 1416 2
-90 1275 3
-90 743 3
-90 3896 4
-90 1429 5
-90 1284 5
-90 3100 3
-90 2302 3
-90 3105 2
-90 3108 3
-90 903 5
-90 1437 3
-90 2094 2
-90 765 2
-90 3114 3
-90 2170 3
-90 1518 3
-90 913 5
-90 2177 2
-90 1449 4
-90 1377 3
-90 2321 3
-90 3129 4
-90 1527 5
-90 1457 4
-90 852 3
-90 1459 3
-90 780 2
-90 784 3
-90 858 5
-90 786 4
-90 2331 2
-90 3208 3
-90 2335 1
-90 2190 4
-90 1390 4
-90 1466 3
-90 861 5
-90 869 3
-90 3141 4
-90 3219 3
-90 3147 4
-90 2273 5
-90 1617 5
-90 2278 5
-90 1476 2
-90 2279 3
-90 2424 3
-90 3082 4
-90 2427 3
-90 1625 4
-90 1552 3
-90 1480 3
-90 1485 4
-90 1487 4
-90 3160 3
-90 2431 2
-90 2505 3
-90 1704 3
-90 3247 2
-90 3175 4
-90 1721 4
-90 3255 3
-90 3256 4
-90 3257 3
-90 110 4
-90 3186 3
-90 112 5
-90 1729 4
-90 1584 3
-90 1586 3
-90 1588 2
-90 986 3
-90 3408 4
-90 3264 4
-90 3265 4
-90 3267 4
-90 2539 4
-90 2394 2
-90 2395 3
-90 993 3
-90 994 3
-90 999 4
-90 2540 4
-90 2542 5
-90 3418 4
-90 1673 3
-90 1747 4
-90 208 3
-90 2628 3
-90 1682 3
-90 141 3
-90 1687 4
-90 2700 4
-90 2707 3
-90 150 3
-90 224 4
-90 1769 4
-90 2712 2
-90 1912 3
-90 1914 4
-90 300 4
-90 1917 3
-90 1845 3
-90 235 4
-90 1779 3
-90 164 4
-90 2723 3
-90 2582 5
-90 3386 5
-90 1783 3
-90 1784 4
-90 1858 5
-90 318 5
-90 2803 3
-90 253 3
-90 1799 3
-90 257 4
-90 185 3
-90 260 3
-90 12 3
-90 337 3
-90 19 4
-90 3623 3
-90 3624 5
-90 2683 5
-90 2688 3
-90 344 4
-90 1020 3
-90 2908 3
-90 2762 4
-90 2763 3
-90 1892 4
-90 350 1
-90 353 3
-90 281 3
-90 32 4
-90 34 3
-90 2912 4
-90 2841 3
-90 2916 4
-90 3578 5
-90 1037 3
-90 294 3
-90 45 3
-90 47 4
-90 1042 4
-90 2858 4
-90 440 4
-90 1049 3
-90 370 5
-90 50 5
-90 373 4
-90 376 3
-90 377 2
-90 3809 3
-90 2791 5
-90 2792 4
-90 527 4
-90 454 3
-90 60 3
-90 383 3
-90 457 4
-90 63 4
-90 1136 5
-90 2879 5
-90 608 5
-90 392 3
-90 539 4
-90 468 3
-90 3821 3
-90 3751 3
-90 3752 3
-90 3826 3
-90 3755 3
-90 2880 5
-90 2881 4
-90 1218 5
-90 1219 3
-90 2959 4
-90 471 3
-90 477 3
-90 2027 3
-90 2028 4
-90 1080 5
-90 551 3
-90 553 4
-90 1089 4
-90 95 3
-90 2106 2
-90 2108 4
-90 1233 3
-90 2976 4
-90 708 3
-91 2997 3
-91 1265 5
-91 1197 5
-91 2073 5
-91 1276 5
-91 1278 5
-91 3104 5
-91 1500 5
-91 3035 5
-91 1294 4
-91 910 5
-91 2174 5
-91 912 5
-91 920 5
-91 921 5
-91 3060 5
-91 1394 5
-91 2418 4
-91 947 5
-91 898 5
-91 110 5
-91 111 3
-91 2396 5
-91 3421 5
-91 3363 5
-91 3386 4
-91 2599 4
-91 3543 5
-91 268 4
-91 21 5
-91 2918 5
-91 514 4
-91 2788 5
-91 2863 4
-91 2791 5
-91 3671 5
-91 1060 4
-91 1079 5
-91 1226 5
-91 1307 5
-91 1238 5
-91 492 4
-91 497 5
-92 648 2
-92 3861 2
-92 2123 3
-92 3863 1
-92 3864 1
-92 2125 5
-92 3793 3
-92 2995 2
-92 2058 3
-92 3798 4
-92 2997 1
-92 653 2
-92 588 5
-92 589 3
-92 1 4
-92 2 2
-92 3004 1
-92 3005 2
-92 6 1
-92 8 3
-92 2133 3
-92 2134 4
-92 1405 1
-92 3948 2
-92 2137 3
-92 2139 4
-92 1265 1
-92 733 4
-92 661 1
-92 1197 5
-92 736 2
-92 1198 3
-92 593 4
-92 594 3
-92 595 5
-92 596 3
-92 1411 3
-92 2141 4
-92 2142 2
-92 3882 2
-92 2143 2
-92 2144 4
-92 2072 4
-92 1270 5
-92 810 1
-92 1271 4
-92 2077 3
-92 2078 3
-92 748 1
-92 2080 5
-92 2081 5
-92 3895 1
-92 2083 5
-92 2084 5
-92 1282 1
-92 3897 3
-92 2085 3
-92 1358 2
-92 1359 1
-92 2089 1
-92 829 4
-92 3033 5
-92 3107 4
-92 3034 3
-92 2161 3
-92 2162 4
-92 900 3
-92 2090 1
-92 901 3
-92 2092 1
-92 2093 4
-92 1291 4
-92 2167 2
-92 2169 2
-92 2096 5
-92 3113 1
-92 3040 4
-92 3114 5
-92 1513 1
-92 910 5
-92 2174 4
-92 914 5
-92 1377 4
-92 1380 5
-92 920 4
-92 2329 3
-92 1527 1
-92 1381 4
-92 2188 1
-92 780 3
-92 1387 2
-92 783 1
-92 2407 3
-92 1605 2
-92 2336 3
-92 1461 4
-92 1608 3
-92 1393 2
-92 2413 5
-92 2414 3
-92 1615 2
-92 3147 3
-92 2273 2
-92 1544 2
-92 3078 1
-92 1479 2
-92 2421 3
-92 2422 2
-92 3225 1
-92 2423 4
-92 3081 2
-92 1623 1
-92 3155 2
-92 3082 1
-92 2353 2
-92 1552 3
-92 2428 2
-92 2355 4
-92 1556 1
-92 2359 1
-92 953 4
-92 886 4
-92 2500 1
-92 2502 5
-92 2430 1
-92 1702 2
-92 1704 2
-92 3238 1
-92 2363 1
-92 2364 1
-92 1562 3
-92 2294 3
-92 1499 1
-92 899 4
-92 1641 5
-92 3247 4
-92 3175 2
-92 3176 2
-92 1573 5
-92 3178 2
-92 104 3
-92 3325 1
-92 3252 2
-92 1721 4
-92 3253 4
-92 1722 2
-92 3255 5
-92 2453 1
-92 3257 4
-92 2455 2
-92 110 5
-92 3185 1
-92 2529 3
-92 1582 3
-92 3189 2
-92 1801 3
-92 2605 2
-92 3408 3
-92 2606 4
-92 3409 4
-92 2394 4
-92 2396 4
-92 1595 1
-92 2398 1
-92 126 3
-92 2541 4
-92 3271 3
-92 3418 3
-92 2470 3
-92 3273 2
-92 2471 2
-92 2546 1
-92 1673 1
-92 208 1
-92 2622 3
-92 1821 1
-92 1750 1
-92 3285 1
-92 1754 2
-92 1682 3
-92 2485 3
-92 2559 1
-92 216 5
-92 217 1
-92 1688 5
-92 2700 4
-92 2702 1
-92 2706 4
-92 1831 2
-92 1760 1
-92 3438 4
-92 2709 3
-92 1907 5
-92 2491 1
-92 2565 4
-92 2566 1
-92 1693 3
-92 223 4
-92 153 3
-92 158 3
-92 3510 3
-92 3511 1
-92 3512 1
-92 2710 1
-92 3513 3
-92 3515 1
-92 2713 1
-92 1911 2
-92 2714 2
-92 2716 4
-92 2717 4
-92 2571 5
-92 2718 1
-92 2572 4
-92 2719 1
-92 1917 4
-92 1848 1
-92 1777 5
-92 1779 3
-92 169 2
-92 2720 2
-92 2722 1
-92 2723 1
-92 3526 4
-92 3453 1
-92 2724 3
-92 3454 1
-92 1923 2
-92 2580 4
-92 2581 4
-92 1784 2
-92 317 3
-92 318 5
-92 2800 1
-92 3604 4
-92 2804 5
-92 3534 2
-92 3608 1
-92 3535 1
-92 3536 1
-92 3466 4
-92 3396 3
-92 3397 4
-92 3398 3
-92 2599 1
-92 329 3
-92 256 2
-92 3615 3
-92 3616 1
-92 3543 1
-92 3617 3
-92 3471 1
-92 3548 4
-92 3477 2
-92 1873 2
-92 1009 3
-92 1947 4
-92 17 4
-92 3623 1
-92 2822 3
-92 3553 2
-92 3554 1
-92 3481 2
-92 3555 1
-92 1951 5
-92 3483 4
-92 2827 2
-92 3484 3
-92 2683 5
-92 1882 1
-92 1019 3
-92 2687 4
-92 2688 3
-92 24 2
-92 2906 1
-92 1022 4
-92 2762 4
-92 1023 1
-92 2763 4
-92 1961 3
-92 1024 2
-92 2694 3
-92 1028 4
-92 1029 2
-92 1967 5
-92 1968 5
-92 32 3
-92 34 3
-92 357 4
-92 36 2
-92 39 4
-92 2840 3
-92 1101 4
-92 3717 3
-92 1030 4
-92 2770 3
-92 3646 1
-92 2771 2
-92 1032 3
-92 2918 4
-92 1033 2
-92 1035 5
-92 3578 5
-92 360 3
-92 434 4
-92 435 4
-92 364 5
-92 292 4
-92 368 4
-92 2858 1
-92 440 3
-92 1047 5
-92 514 3
-92 376 1
-92 377 2
-92 2861 1
-92 2791 4
-92 3594 3
-92 1127 2
-92 1057 2
-92 2797 5
-92 380 3
-92 1997 2
-92 527 1
-92 455 3
-92 60 1
-92 3743 1
-92 2005 5
-92 2006 3
-92 3675 5
-92 2874 4
-92 1136 5
-92 2876 2
-92 1064 1
-92 465 1
-92 73 2
-92 76 2
-92 3821 1
-92 2011 5
-92 3824 1
-92 3751 4
-92 3825 3
-92 3752 1
-92 2950 2
-92 3826 2
-92 3753 4
-92 2015 4
-92 1214 3
-92 2881 2
-92 2018 2
-92 1073 4
-92 2959 5
-92 2888 2
-92 616 2
-92 86 1
-92 2028 2
-92 1080 5
-92 551 2
-92 552 4
-92 480 4
-92 1088 4
-92 486 3
-92 2102 3
-92 2033 1
-92 1307 5
-92 3776 1
-92 2038 5
-92 2978 3
-92 709 3
-92 1099 3
-92 569 3
-92 2114 4
-92 3927 4
-92 1240 2
-92 2043 4
-92 3785 4
-92 2046 5
-92 2048 3
-93 648 4
-93 1320 3
-93 2990 4
-93 3793 5
-93 2991 4
-93 720 5
-93 2058 5
-93 653 4
-93 588 3
-93 589 5
-93 6 4
-93 1405 1
-93 2137 4
-93 3877 1
-93 2139 5
-93 733 3
-93 1197 5
-93 736 3
-93 1198 4
-93 667 1
-93 595 3
-93 2143 4
-93 741 5
-93 3889 2
-93 1275 4
-93 2153 1
-93 1429 1
-93 1356 3
-93 3107 3
-93 2161 3
-93 2162 1
-93 1291 4
-93 2167 4
-93 1438 3
-93 839 1
-93 2174 4
-93 1372 4
-93 1373 3
-93 1375 3
-93 1377 4
-93 1378 2
-93 3054 1
-93 1385 3
-93 780 3
-93 1387 1
-93 1388 2
-93 786 2
-93 2404 3
-93 2405 3
-93 3063 2
-93 1606 1
-93 2409 3
-93 1608 2
-93 2193 5
-93 2194 3
-93 861 3
-93 3213 5
-93 1610 4
-93 2273 4
-93 1544 1
-93 2278 2
-93 1479 3
-93 2422 1
-93 3082 1
-93 1552 2
-93 2355 4
-93 1556 1
-93 2288 3
-93 1702 3
-93 1499 2
-93 2373 2
-93 1573 3
-93 2376 4
-93 3257 3
-93 2528 4
-93 112 1
-93 1586 3
-93 1587 4
-93 2532 1
-93 2533 1
-93 3267 4
-93 1591 2
-93 1599 1
-93 2616 1
-93 2549 1
-93 2476 4
-93 204 2
-93 208 3
-93 1752 1
-93 2628 3
-93 1681 2
-93 1688 2
-93 145 2
-93 2490 2
-93 1907 4
-93 3439 1
-93 1690 3
-93 1769 2
-93 153 1
-93 3440 1
-93 2640 3
-93 3444 3
-93 2642 2
-93 2643 2
-93 2571 5
-93 1918 2
-93 163 4
-93 165 1
-93 3452 2
-93 2723 3
-93 3527 3
-93 316 3
-93 172 4
-93 173 3
-93 2807 1
-93 2808 3
-93 2735 4
-93 1792 2
-93 329 3
-93 2815 2
-93 2816 1
-93 2818 1
-93 3479 4
-93 405 1
-93 10 3
-93 260 4
-93 198 3
-93 3623 1
-93 1954 4
-93 21 3
-93 349 2
-93 3702 5
-93 3703 4
-93 3704 4
-93 2763 4
-93 3639 4
-93 2692 3
-93 420 1
-93 1967 5
-93 353 4
-93 288 1
-93 2916 4
-93 502 1
-93 1036 4
-93 1037 4
-93 434 3
-93 364 3
-93 292 4
-93 44 2
-93 368 4
-93 2924 2
-93 442 2
-93 377 3
-93 1127 4
-93 1129 2
-93 2797 5
-93 380 2
-93 457 3
-93 2000 3
-93 3740 3
-93 2002 3
-93 1201 4
-93 2005 4
-93 2006 2
-93 2944 5
-93 2949 4
-93 466 3
-93 3751 5
-93 3753 3
-93 3827 5
-93 2015 5
-93 1214 4
-93 2019 5
-93 1218 3
-93 1073 1
-93 474 5
-93 2028 5
-93 3697 3
-93 3698 3
-93 480 1
-93 555 2
-93 485 4
-93 95 1
-93 3911 2
-93 2105 5
-93 1233 4
-93 2115 3
-93 1240 5
-94 3933 3
-94 1321 3
-94 1258 3
-94 3018 5
-94 1349 3
-94 2085 3
-94 908 4
-94 2288 5
-94 2377 5
-94 2455 4
-94 2716 4
-94 1961 4
-94 3576 4
-94 1994 2
-94 2003 5
-94 1130 4
-94 3837 3
-94 551 5
-94 3917 4
-94 2118 4
-95 1249 4
-95 1179 1
\ No newline at end of file
Index: model.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># -*- coding: utf-8 -*-\r\n# @Time    : 2019/4/22 15:07\r\n# @Author  : Jason\r\n# @FileName: model.py\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nclass Config(object):\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    vec_dim = 128  # \r\n    num_classes = 5\r\n    num_filters = 256  # \r\n    kernel_size = 5  # \r\n    hidden_dim = 64  # \r\n\r\n    class_num = 5\r\n\r\n    dropout_keep_prob = 0.5  # dropout\r\n    learning_rate = 0.1  # \r\n\r\n    batch_size = 64  # iterator64\r\n    num_epochs = 500  # \r\n\r\n    print_per_batch = 10  # \r\n    save_per_batch = 10  # tensorboard\r\n\r\n\r\nclass GMCNN(object):\r\n    def __init__(self, config):\r\n        self.config = config\r\n        # \r\n        # xshape[batch_size, vec_dim,vec_dim]\r\n        self.input_x = tf.placeholder(tf.float32, [None, self.config.vec_dim*2], name=\"input_x\")\r\n        self.input_y = tf.placeholder(tf.float32, [None, self.config.num_classes], name=\"input_y\")\r\n        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\r\n\r\n        # self.cnn()\r\n        self.gmnn()\r\n\r\n    # def cnn(self):\r\n    #     \"\"\"\r\n    #     CNN\r\n    #     :return:\r\n    #     \"\"\"\r\n    #     with tf.name_scope(\"cnn\"):\r\n    #         # CNN layer\r\n    #         conv1 = tf.layers.conv1d(self.input_x, 64, self.config.kernel_size, activation=\"relu\", name=\"conv1\")\r\n    #         print(\"conv1: \", conv1.shape)\r\n    #         # global max pooling layer\r\n    #         gmp1 = tf.layers.max_pooling1d(conv1, pool_size=4, strides=2, name='gmp1')\r\n    #         print(\"gmp1: \", gmp1.shape)\r\n    #\r\n    #         conv2 = tf.layers.conv1d(gmp1, 128, self.config.kernel_size, activation=\"relu\", name=\"conv2\")\r\n    #         print(\"conv2: \", conv2.shape)\r\n    #         gmp2 = tf.layers.max_pooling1d(conv2, pool_size=4, strides=2, name=\"gmp2\")\r\n    #         print(\"gmp2: \", gmp2.shape)\r\n    #\r\n    #         conv3 = tf.layers.conv1d(gmp2, self.config.num_filters, self.config.kernel_size, activation=\"relu\",\r\n    #                                  name=\"conv3\")\r\n    #         print(\"conv3: \", conv3.shape)\r\n    #         gmp3 = tf.layers.max_pooling1d(conv3, pool_size=4, strides=2, name=\"gmp3\")\r\n    #         print(\"gmp3: \", gmp3.shape)\r\n    #         gmp = tf.reduce_max(gmp3, reduction_indices=[1], name=\"gmp\")\r\n    #\r\n    #     with tf.name_scope(\"score\"):\r\n    #         # dropoutrelu\r\n    #         fc = tf.layers.dense(gmp, self.config.hidden_dim, name=\"fc1\")\r\n    #         fc = tf.contrib.layers.dropout(fc, self.config.dropout_keep_prob)\r\n    #         fc = tf.nn.relu(fc)\r\n    #         print(\"fc: \", fc.shape)\r\n    #\r\n    #         # \r\n    #         self.logits = tf.layers.dense(fc, self.config.num_classes, name=\"fc2\")\r\n    #         self.y_pred_class = tf.argmax(tf.nn.softmax(self.logits), 1)  # softmaxone-hot,\r\n    #\r\n    #     with tf.name_scope(\"optimize\"):\r\n    #         # \r\n    #         cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.input_y)\r\n    #         self.loss = tf.reduce_mean(cross_entropy)\r\n    #         # \r\n    #         self.optimizer = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\r\n    #\r\n    #     with tf.name_scope(\"accuracy\"):\r\n    #         # \r\n    #         correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_class)\r\n    #         self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n    def gmnn(self):\r\n        with tf.name_scope(\"concate\"):\r\n            con = tf.layers.dense(self.input_x, self.config.num_classes, name=\"con\")\r\n            con = tf.layers.dropout(con, self.config.dropout_keep_prob)\r\n            con = tf.nn.relu(con)\r\n        with tf.name_scope(\"score\"):\r\n\r\n            self.logits = tf.layers.dense(tf.nn.sigmoid(self.input_x), self.config.num_classes, name=\"fc\")\r\n            self.y_pred_class = tf.argmax(tf.nn.softmax(self.logits), 1)\r\n        with tf.name_scope(\"optimize\"):\r\n            # \r\n            cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.input_y)\r\n            self.loss = tf.reduce_mean(cross_entropy)\r\n            # \r\n            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\r\n\r\n        with tf.name_scope(\"accuracy\"):\r\n            # \r\n            correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_class)\r\n            self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- model.py	(date 1558009865000)
+++ model.py	(date 1557712382405)
@@ -34,77 +34,50 @@
         self.config = config
         # 
         # xshape[batch_size, vec_dim,vec_dim]
-        self.input_x = tf.placeholder(tf.float32, [None, self.config.vec_dim*2], name="input_x")
-        self.input_y = tf.placeholder(tf.float32, [None, self.config.num_classes], name="input_y")
+        self.input_x = tf.placeholder(tf.float32, [None, self.config.vec_dim, self.config.vec_dim], name="input_x")
+        self.input_y = tf.placeholder(tf.float32, [None, 1], name="input_y")
         self.keep_prob = tf.placeholder(tf.float32, name="keep_prob")
 
-        # self.cnn()
-        self.gmnn()
+        self.cnn()
+
+    def cnn(self):
+        """
+        CNN
+        :return:
+        """
+        with tf.name_scope("cnn"):
+            # CNN layer
+            conv1 = tf.layers.conv1d(self.input_x, 64, self.config.kernel_size, activation="relu", name="conv1")
+            print("conv1: ", conv1.shape)
+            # global max pooling layer
+            gmp1 = tf.layers.max_pooling1d(conv1, pool_size=4, strides=2, name='gmp1')
+            print("gmp1: ", gmp1.shape)
 
-    # def cnn(self):
-    #     """
-    #     CNN
-    #     :return:
-    #     """
-    #     with tf.name_scope("cnn"):
-    #         # CNN layer
-    #         conv1 = tf.layers.conv1d(self.input_x, 64, self.config.kernel_size, activation="relu", name="conv1")
-    #         print("conv1: ", conv1.shape)
-    #         # global max pooling layer
-    #         gmp1 = tf.layers.max_pooling1d(conv1, pool_size=4, strides=2, name='gmp1')
-    #         print("gmp1: ", gmp1.shape)
-    #
-    #         conv2 = tf.layers.conv1d(gmp1, 128, self.config.kernel_size, activation="relu", name="conv2")
-    #         print("conv2: ", conv2.shape)
-    #         gmp2 = tf.layers.max_pooling1d(conv2, pool_size=4, strides=2, name="gmp2")
-    #         print("gmp2: ", gmp2.shape)
-    #
-    #         conv3 = tf.layers.conv1d(gmp2, self.config.num_filters, self.config.kernel_size, activation="relu",
-    #                                  name="conv3")
-    #         print("conv3: ", conv3.shape)
-    #         gmp3 = tf.layers.max_pooling1d(conv3, pool_size=4, strides=2, name="gmp3")
-    #         print("gmp3: ", gmp3.shape)
-    #         gmp = tf.reduce_max(gmp3, reduction_indices=[1], name="gmp")
-    #
-    #     with tf.name_scope("score"):
-    #         # dropoutrelu
-    #         fc = tf.layers.dense(gmp, self.config.hidden_dim, name="fc1")
-    #         fc = tf.contrib.layers.dropout(fc, self.config.dropout_keep_prob)
-    #         fc = tf.nn.relu(fc)
-    #         print("fc: ", fc.shape)
-    #
-    #         # 
-    #         self.logits = tf.layers.dense(fc, self.config.num_classes, name="fc2")
-    #         self.y_pred_class = tf.argmax(tf.nn.softmax(self.logits), 1)  # softmaxone-hot,
-    #
-    #     with tf.name_scope("optimize"):
-    #         # 
-    #         cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.input_y)
-    #         self.loss = tf.reduce_mean(cross_entropy)
-    #         # 
-    #         self.optimizer = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)
-    #
-    #     with tf.name_scope("accuracy"):
-    #         # 
-    #         correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_class)
-    #         self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
-    def gmnn(self):
-        with tf.name_scope("concate"):
-            con = tf.layers.dense(self.input_x, self.config.num_classes, name="con")
-            con = tf.layers.dropout(con, self.config.dropout_keep_prob)
-            con = tf.nn.relu(con)
+            conv2 = tf.layers.conv1d(gmp1, 128, self.config.kernel_size, activation="relu", name="conv2")
+            print("conv2: ", conv2.shape)
+            gmp2 = tf.layers.max_pooling1d(conv2, pool_size=4, strides=2, name="gmp2")
+            print("gmp2: ", gmp2.shape)
+
+            conv3 = tf.layers.conv1d(gmp2, self.config.num_filters, self.config.kernel_size, activation="relu",
+                                     name="conv3")
+            print("conv3: ", conv3.shape)
+            gmp3 = tf.layers.max_pooling1d(conv3, pool_size=4, strides=2, name="gmp3")
+            print("gmp3: ", gmp3.shape)
+            gmp = tf.reduce_max(gmp3, reduction_indices=[1], name="gmp")
+
         with tf.name_scope("score"):
+            # dropoutrelu
+            fc = tf.layers.dense(gmp, self.config.hidden_dim, name="fc1")
+            fc = tf.contrib.layers.dropout(fc, self.config.dropout_keep_prob)
+            fc = tf.nn.relu(fc)
+            print("fc: ", fc.shape)
 
-            self.logits = tf.layers.dense(tf.nn.sigmoid(self.input_x), self.config.num_classes, name="fc")
-            self.y_pred_class = tf.argmax(tf.nn.softmax(self.logits), 1)
+            # 
+            self.pred_rating = tf.layers.dense(fc, 1, name="fc2")
+
         with tf.name_scope("optimize"):
             # 
-            cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.input_y)
-            self.loss = tf.reduce_mean(cross_entropy)
+            absolute_loss = tf.abs(tf.subtract(self.input_y, self.pred_rating))
+            self.loss = tf.reduce_mean(absolute_loss)
             # 
             self.optimizer = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)
-
-        with tf.name_scope("accuracy"):
-            # 
-            correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_class)
-            self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
Index: run_model.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># -*- coding: utf-8 -*-\r\n# @Time    : 2019/4/24 16:49\r\n# @Author  : Jason\r\n# @FileName: run_model.py\r\n\r\nimport sys\r\nfrom model import Config, GMCNN\r\nimport os\r\nimport tensorflow as tf\r\nimport time\r\nfrom utils import preprocess, getUsersEdge, getMoviesEdge, batch_iter\r\nfrom datetime import timedelta\r\n\r\n\r\ndef getTimeDif(time):\r\n    return timedelta(seconds=int(round(time)))\r\n\r\n\r\ndef feed_data(x_batch, y_batch, dropout_keep_prob):\r\n    # print(\"x_batch: \", x_batch.shape)\r\n    # print(\"y_batch: \", y_batch.shape)\r\n    feed_dict = {\r\n        model.input_x: x_batch,\r\n        model.input_y: y_batch,\r\n        model.keep_prob: dropout_keep_prob\r\n    }\r\n    return feed_dict\r\n\r\n\r\ndef evaluate(session, x_val, y_val):\r\n    \"\"\"\"\"\"\r\n    data_len = len(x_val)\r\n    batch_eval = batch_iter(x_val, y_val, model.config.batch_size)\r\n    total_loss = 0.0\r\n    total_acc = 0.0\r\n    for x_batch, y_batch in batch_eval:\r\n        batch_len = len(x_batch)\r\n        feed_dict = feed_data(x_batch, y_batch, model.config.dropout_keep_prob)\r\n        loss, acc = session.run([model.loss, model.accuracy], feed_dict=feed_dict)\r\n        total_loss += loss * batch_len\r\n        total_acc += acc * batch_len\r\n\r\n    return total_loss / data_len, total_acc / data_len\r\n\r\n\r\ndef train():\r\n    print(\"Configuring TensorBoard and Saver...\")\r\n    # tensorboard\r\n    tensorboard_dir = './tensorboard/GMCNN'\r\n    if not os.path.exists(tensorboard_dir):\r\n        os.mkdir(tensorboard_dir)\r\n    tf.summary.scalar(\"loss\", model.loss)\r\n    tf.summary.scalar('accuracy', model.accuracy)\r\n    merged_summary = tf.summary.merge_all()\r\n    writer = tf.summary.FileWriter(tensorboard_dir)\r\n\r\n    # saver\r\n    saver_dir = './saver/GMCNN'\r\n    saver = tf.train.Saver()\r\n    if not os.path.exists(saver_dir):\r\n        os.mkdir(saver_dir)\r\n\r\n    print(\"Successfully configure tensorboard and saver.\\n\")\r\n\r\n    print(\"Loading training and validation data...\")\r\n    user_vec_dir = './emb/users.emb'\r\n    movie_vec_dir = './emb/movies.emb'\r\n    if not os.path.exists(user_vec_dir):\r\n        getUsersEdge(users_dir, user_vec_dir)\r\n    if not os.path.exists(movie_vec_dir):\r\n        getMoviesEdge(movies_dir, movie_vec_dir)\r\n\r\n    # \r\n    start_time = time.time()\r\n    x_train, y_train = preprocess(user_vec_dir, movie_vec_dir, ratings_edgelist_dir, user_edge_dir, movie_edge_dir)\r\n    x_val, y_val = preprocess(user_vec_dir, movie_vec_dir, ratings_val_edgelist_dir, user_edge_dir, movie_edge_dir)\r\n    print(\"Training data x: \", x_train.shape)\r\n    print(\"Training data x: \", x_train)\r\n    print(\"Training data y: \", y_train.shape)\r\n    print(\"Training data y: \", y_train)\r\n    time_dif = time.time() - start_time\r\n    print(\"Time usage: \", getTimeDif(time_dif))\r\n    print(\"Successfully load training and evaluating data.\\n\")\r\n\r\n    # session\r\n    session = tf.Session()\r\n    session.run(tf.global_variables_initializer())\r\n    writer.add_graph(session.graph)\r\n\r\n    print(\"Training and evaluating...\")\r\n    start_time = time.time()\r\n    total_batch = 0  # \r\n    best_acc_val = 0.0  # \r\n    last_improved = 0  # \r\n    require_improvement = 1000  # 1000\r\n\r\n    flag = False\r\n    for epoch in range(config.num_epochs):\r\n        print(\"Epoch :  \", epoch + 1)\r\n        batch_train = batch_iter(x_train, y_train, config.batch_size)\r\n        for x_batch, y_batch in batch_train:\r\n            feed_dict = feed_data(x_batch, y_batch, config.dropout_keep_prob)\r\n\r\n            if total_batch % config.save_per_batch == 0:\r\n                # tensorboard scalar\r\n                s = session.run(merged_summary, feed_dict=feed_dict)\r\n                writer.add_summary(s, total_batch)\r\n\r\n            if total_batch % config.print_per_batch == 0:\r\n                # \r\n                feed_dict[model.keep_prob] = 1.0\r\n                loss_train, acc_train = session.run([model.loss, model.accuracy], feed_dict=feed_dict)\r\n                loss_val, acc_val = evaluate(session, x_val, y_val)  # todo\r\n\r\n                if acc_val > best_acc_val:\r\n                    # \r\n                    best_acc_val = acc_val\r\n                    last_improved = total_batch\r\n                    saver.save(sess=session, save_path=saver_dir)\r\n                    improved_str = '*'\r\n                else:\r\n                    improved_str = ''\r\n\r\n                time_dif = getTimeDif(time.time() - start_time)\r\n                msg = 'iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\r\n                      + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\r\n                print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\r\n\r\n            session.run(model.optimizer, feed_dict=feed_dict)  # \r\n            total_batch += 1\r\n\r\n            if total_batch - last_improved > require_improvement:\r\n                # \r\n                print(\"No optimization for a long time, auto-stopping...\")\r\n                flag = True\r\n                break  # \r\n        if flag:  # \r\n            break\r\n\r\n\r\n# def test():\r\n#     print(\"Loading test data...\")\r\n#     start_time = time.time()\r\n#     x_test, y_test = process_file(test_dir, word_to_id, cat_to_id, config.seq_length)\r\n#\r\n#     session = tf.Session()\r\n#     session.run(tf.global_variables_initializer())\r\n#     saver = tf.train.Saver()\r\n#     saver.restore(sess=session, save_path=save_path)  # \r\n#\r\n#     print('Testing...')\r\n#     loss_test, acc_test = evaluate(session, x_test, y_test)\r\n#     msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\r\n#     print(msg.format(loss_test, acc_test))\r\n#\r\n#     batch_size = 128\r\n#     data_len = len(x_test)\r\n#     num_batch = int((data_len - 1) / batch_size) + 1\r\n#\r\n#     y_test_cls = np.argmax(y_test, 1)\r\n#     y_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # \r\n#     for i in range(num_batch):  # \r\n#         start_id = i * batch_size\r\n#         end_id = min((i + 1) * batch_size, data_len)\r\n#         feed_dict = {\r\n#             model.input_x: x_test[start_id:end_id],\r\n#             model.keep_prob: 1.0\r\n#         }\r\n#         y_pred_cls[start_id:end_id] = session.run(model.y_pred_class, feed_dict=feed_dict)\r\n#\r\n#     # \r\n#     print(\"Precision, Recall and F1-Score...\")\r\n#     print(metrics.classification_report(y_test_cls, y_pred_cls, target_names=categories))\r\n#\r\n#     # \r\n#     print(\"Confusion Matrix...\")\r\n#     cm = metrics.confusion_matrix(y_test_cls, y_pred_cls)\r\n#     print(cm)\r\n#\r\n#     time_dif = get_time_dif(start_time)\r\n#     print(\"Time usage:\", time_dif)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n    if len(sys.argv) != 2 or sys.argv[1] not in ['train', 'test']:\r\n        raise ValueError(\"\"\"usage: python run_model.py train/test\"\"\")\r\n    users_dir = './data/users.txt'\r\n    movies_dir = './data/movies.txt'\r\n    ratings_edgelist_dir = './graph/ratings_small.edge'\r\n    ratings_val_edgelist_dir = './graph/ratings_val.edge'\r\n    user_edge_dir = \"./graph/users.edge\"\r\n    movie_edge_dir = \"./graph/movies.edge\"\r\n    print(\"Configuring GMCNN model...\")\r\n    config = Config()\r\n    print(\"Successfully configure model.\\n\")\r\n    model = GMCNN(config)\r\n    if sys.argv[1] == 'train':\r\n        train()\r\n    # else:\r\n    # test()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- run_model.py	(date 1558009865000)
+++ run_model.py	(date 1557712382410)
@@ -36,11 +36,11 @@
     for x_batch, y_batch in batch_eval:
         batch_len = len(x_batch)
         feed_dict = feed_data(x_batch, y_batch, model.config.dropout_keep_prob)
-        loss, acc = session.run([model.loss, model.accuracy], feed_dict=feed_dict)
+        loss = session.run([model.loss], feed_dict=feed_dict)
         total_loss += loss * batch_len
-        total_acc += acc * batch_len
+        # total_acc += acc * batch_len
 
-    return total_loss / data_len, total_acc / data_len
+    return total_loss / data_len
 
 
 def train():
@@ -50,7 +50,7 @@
     if not os.path.exists(tensorboard_dir):
         os.mkdir(tensorboard_dir)
     tf.summary.scalar("loss", model.loss)
-    tf.summary.scalar('accuracy', model.accuracy)
+    # tf.summary.scalar('accuracy', model.accuracy)
     merged_summary = tf.summary.merge_all()
     writer = tf.summary.FileWriter(tensorboard_dir)
 
@@ -90,7 +90,7 @@
     print("Training and evaluating...")
     start_time = time.time()
     total_batch = 0  # 
-    best_acc_val = 0.0  # 
+    best_loss_val = 1.0  # 
     last_improved = 0  # 
     require_improvement = 1000  # 1000
 
@@ -109,12 +109,12 @@
             if total_batch % config.print_per_batch == 0:
                 # 
                 feed_dict[model.keep_prob] = 1.0
-                loss_train, acc_train = session.run([model.loss, model.accuracy], feed_dict=feed_dict)
-                loss_val, acc_val = evaluate(session, x_val, y_val)  # todo
+                loss_train = session.run([model.loss], feed_dict=feed_dict)
+                loss_val = evaluate(session, x_val, y_val)  # todo
 
-                if acc_val > best_acc_val:
+                if loss_val < best_loss_val:
                     # 
-                    best_acc_val = acc_val
+                    best_loss_val = loss_val
                     last_improved = total_batch
                     saver.save(sess=session, save_path=saver_dir)
                     improved_str = '*'
@@ -122,9 +122,11 @@
                     improved_str = ''
 
                 time_dif = getTimeDif(time.time() - start_time)
-                msg = 'iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \
-                      + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'
-                print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))
+                # msg = 'iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \
+                #       + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'
+                # print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))
+                msg = 'iter: {0:>6}, Train Loss: {1:>6.2}, Val Loss: {2:>6.2}, Time: {3} {4}'
+                print(msg.format(total_batch, loss_train, loss_val, time_dif, improved_str))
 
             session.run(model.optimizer, feed_dict=feed_dict)  # 
             total_batch += 1
Index: utils.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># -*- coding: utf-8 -*-\r\n# @Time    : 2019/4/22 16:47\r\n# @Author  : Jason\r\n# @FileName: utils.py\r\nimport numpy as np\r\n\r\n\r\ndef getUsersEdge(user_dir, user_egdelist_dir):\r\n    userID, genders, ages, occupations = [], [], [], []\r\n    with open(user_dir, 'r', encoding='utf-8') as lines:\r\n        for line in lines:\r\n            userID.append(line.strip().split(\"::\")[0])\r\n            genders.append(line.strip().split(\"::\")[1])\r\n            ages.append(line.strip().split(\"::\")[2])\r\n            occupations.append(line.strip().split(\"::\")[3])\r\n    lines.close()\r\n    with open(user_egdelist_dir, 'w', encoding='utf-8') as user_writer:\r\n        for i in range(len(userID)):\r\n            for j in range(len(userID)):\r\n                if i != j and genders[i] == genders[j] and ages[i] == ages[j] and occupations[i] == occupations[j]:\r\n                    user_writer.write(str(userID[i]) + \" \" + str(userID[j]) + \"\\n\")\r\n    user_writer.close()\r\n\r\n\r\ndef cmptype(type1, type2):\r\n    for i in type1:\r\n        if i in type2:\r\n            return True\r\n    return False\r\n\r\n\r\ndef getMoviesEdge(movie_dir, movie_edgelist_dir):\r\n    movieID, types = [], []\r\n    with open(movie_dir, 'r', encoding='utf-8') as lines:\r\n        for line in lines:\r\n            movieID.append(line.strip().split(\"::\")[0])\r\n            types.append(line.strip().split(\"::\")[-1].split(\"|\"))\r\n    lines.close()\r\n    with open(movie_edgelist_dir, 'w', encoding='utf-8') as movie_writer:\r\n        for i in range(len(movieID)):\r\n            for j in range(len(movieID)):\r\n                if i != j and types[i] == types[j]:\r\n                    movie_writer.write(str(movieID[i]) + \" \" + str(movieID[j]) + \"\\n\")\r\n    movie_writer.close()\r\n\r\n\r\ndef getRatingEdge(user_vec_dir, movie_vec_dir, ratings_dir, training_data_dir):\r\n    flag = False\r\n    user_dic = {}\r\n    with open(user_vec_dir, 'r', encoding='utf-8') as lines:\r\n        for line in lines:\r\n            if flag:\r\n                user_node = line.split(\" \")[0]\r\n                user_node_vec = list(map(float, line.split(\" \")[1:]))\r\n                if user_node not in user_dic:\r\n                    user_dic[user_node] = user_node_vec\r\n                else:\r\n                    continue\r\n            else:\r\n                flag = True\r\n    lines.close()\r\n\r\n    flag = False\r\n    movie_dic = {}\r\n    with open(movie_vec_dir, 'r', encoding='utf-8') as lines:\r\n        for line in lines:\r\n            if flag:\r\n                movie_node = line.split(\" \")[0]\r\n                movie_node_vec = list(map(float, line.split(\" \")[1:]))\r\n                if movie_node not in movie_dic:\r\n                    movie_dic[movie_node] = movie_node_vec\r\n                else:\r\n                    continue\r\n            else:\r\n                flag = True\r\n    lines.close()\r\n    with open(training_data_dir, 'w', encoding='utf-8') as writer:\r\n        with open(ratings_dir, 'r', encoding='utf-8') as lines:\r\n            for line in lines:\r\n                userdID = line.split(\"::\")[0]\r\n                movieID = line.split(\"::\")[1]\r\n                rating = line.split(\"::\")[2]\r\n                if userdID in user_dic.keys() and movieID in movie_dic.keys():\r\n                    user_dic[userdID].extend(movie_dic[movieID])\r\n                    res = \"\"\r\n                    for i in user_dic[userdID]:\r\n                        res += str(i) + \" \"\r\n                    writer.write(res + str(rating) + \"\\n\")\r\n        lines.close()\r\n    writer.close()\r\n\r\n\r\ndef preprocess(user_vec_dir, movie_vec_dir, ratings_edge_dir, user_edge_dir, movie_edge_dir):\r\n    flag = False\r\n    user_dic = {}\r\n\r\n    with open(user_vec_dir, 'r', encoding='utf-8') as lines:\r\n        for line in lines:\r\n            if flag:\r\n                user_node = line.strip().split(\" \")[0]\r\n                user_node_vec = np.array(list(map(float, line.strip().split(\" \")[1:])))\r\n                if user_node not in user_dic:\r\n                    user_dic[user_node] = user_node_vec\r\n                else:\r\n                    continue\r\n            else:\r\n                flag = True\r\n    lines.close()\r\n\r\n    flag = False\r\n    movie_dic = {}\r\n    with open(movie_vec_dir, 'r', encoding='utf-8') as lines:\r\n        for line in lines:\r\n            if flag:\r\n                movie_node = line.strip().split(\" \")[0]\r\n                movie_node_vec = np.array(list(map(float, line.split(\" \")[1:])))\r\n                if movie_node not in movie_dic:\r\n                    movie_dic[movie_node] = movie_node_vec\r\n                else:\r\n                    continue\r\n            else:\r\n                flag = True\r\n    lines.close()\r\n\r\n    user_user_dic = {}\r\n    with open(user_edge_dir, 'r', encoding='utf-8') as lines:\r\n        for line in lines:\r\n            user1 = line.strip().split(\" \")[0]\r\n            user2 = line.strip().split(\" \")[1]\r\n            if user1 not in user_user_dic:\r\n                user_user_dic[user1] = [user2]\r\n            else:\r\n                user_user_dic[user1].append(user2)\r\n    lines.close()\r\n\r\n    movie_movie_dic = {}\r\n    with open(movie_edge_dir, 'r', encoding='utf-8') as lines:\r\n        for line in lines:\r\n            movie1 = line.strip().split(\" \")[0]\r\n            movie2 = line.strip().split(\" \")[1]\r\n            if movie1 not in movie_movie_dic:\r\n                movie_movie_dic[movie1] = [movie2]\r\n            else:\r\n                movie_movie_dic[movie1].append(movie2)\r\n    lines.close()\r\n\r\n    train_x = np.zeros((128, 128))\r\n    train_y = []\r\n    user2movie_dic = {}\r\n    movie2user_dic = {}\r\n    with open(ratings_edge_dir, 'r', encoding='utf-8') as lines:\r\n        for user_line in lines:\r\n            userID_u = user_line.strip().split(\" \")[0]\r\n            movieID_u = user_line.strip().split(\" \")[1]\r\n            if userID_u not in user2movie_dic:\r\n                user2movie_dic[userID_u] = [movieID_u]\r\n            else:\r\n                user2movie_dic[userID_u].append(movieID_u)\r\n    lines.close()\r\n    with open(ratings_edge_dir, 'r', encoding='utf-8') as lines:\r\n        for movie_line in lines:\r\n            userID_m = movie_line.strip().split(\" \")[0]\r\n            movieID_m = movie_line.strip().split(\" \")[1]\r\n            if movieID_m not in movie2user_dic:\r\n                movie2user_dic[movieID_m] = [userID_m]\r\n            else:\r\n                movie2user_dic[movieID_m].append(userID_m)\r\n    lines.close()\r\n    with open(ratings_edge_dir, 'r', encoding='utf-8') as lines:\r\n        for line in lines:\r\n            userID = line.strip().split(\" \")[0]\r\n            movieID = line.strip().split(\" \")[1]\r\n            rating = line.strip().split(\" \")[2]\r\n            if userID in user_dic.keys() and movieID in movie_dic.keys():\r\n                i = 0\r\n                while i < 128:\r\n                    if userID in user_user_dic.keys():\r\n                        for con_user in user_user_dic[userID]:\r\n                            if con_user in user2movie_dic.keys():  # \r\n                                if movieID in user2movie_dic[con_user] and i<128:\r\n                                    train_x[i] = list(map(float, user_dic[con_user]))\r\n                                    i += 1\r\n                    if movieID in movie_movie_dic.keys():\r\n                        for con_movie in movie_movie_dic[movieID]:\r\n                            if con_movie in movie2user_dic.keys():\r\n                                if userID in movie2user_dic[con_movie] and i<128:   # \r\n                                    train_x[i] = list(map(float, movie_dic[con_movie]))\r\n                                    i += 1\r\n                temp = [0, 0, 0, 0, 0]\r\n                temp[int(rating) - 1] = 1\r\n                train_y.append(temp)\r\n    lines.close()\r\n\r\n    x_train = np.array(train_x)\r\n    y_train = np.array(train_y)\r\n    return x_train, y_train\r\n\r\n\r\ndef batch_iter(x, y, batch_size=64):\r\n    \"\"\"\"\"\"\r\n    data_len = len(x)\r\n    # print(\"x_len: \", data_len)\r\n    num_batch = int((data_len - 1) / batch_size) + 1\r\n\r\n    indices = np.random.permutation(np.arange(data_len))\r\n    x_shuffle = x[indices]\r\n    y_shuffle = y[indices]\r\n\r\n    for i in range(num_batch):\r\n        start_id = i * batch_size\r\n        end_id = min((i + 1) * batch_size, data_len)\r\n        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    user_vec_dir = \"./emb/users.emb\"\r\n    movie_vec_dir = \"./emb/movies.emb\"\r\n    rating_edge_dir = \"./graph/ratings.edge\"\r\n    rating_dir = \"./data/ratings.txt\"\r\n    training_data_dir = \"./data/traing.txt\"\r\n    user_dir = \"./data/users.txt\"\r\n    user_edge_dir = \"./graph/users.edge\"\r\n    movie_dir = \"./data/movies.txt\"\r\n    movie_edge_dir = \"./graph/movies.edge\"\r\n    # getUsersEdge(user_dir, user_edge_dir)\r\n    # getMoviesEdge(movie_dir, movie_edge_dir)\r\n\r\n    x_train, y_train = preprocess(user_vec_dir, movie_vec_dir, rating_edge_dir, user_edge_dir, movie_edge_dir)\r\n    print(\"x_train: \", x_train)\r\n    print(\"y_train: \", y_train)\r\n\r\n    # getRatingEdge(user_vec_dir, movie_vec_dir, rating_dir, training_data_dir)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- utils.py	(date 1558009865000)
+++ utils.py	(date 1557711592922)
@@ -177,18 +177,16 @@
                     if userID in user_user_dic.keys():
                         for con_user in user_user_dic[userID]:
                             if con_user in user2movie_dic.keys():  # 
-                                if movieID in user2movie_dic[con_user] and i<128:
+                                if movieID in user2movie_dic[con_user] and i < 128:
                                     train_x[i] = list(map(float, user_dic[con_user]))
                                     i += 1
                     if movieID in movie_movie_dic.keys():
                         for con_movie in movie_movie_dic[movieID]:
                             if con_movie in movie2user_dic.keys():
-                                if userID in movie2user_dic[con_movie] and i<128:   # 
+                                if userID in movie2user_dic[con_movie] and i < 128:  # 
                                     train_x[i] = list(map(float, movie_dic[con_movie]))
                                     i += 1
-                temp = [0, 0, 0, 0, 0]
-                temp[int(rating) - 1] = 1
-                train_y.append(temp)
+                train_y.append([int(rating)])
     lines.close()
 
     x_train = np.array(train_x)
